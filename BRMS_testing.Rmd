---
title: "BRMS testing"
author: "Cy Sonkkila"
date: "19/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## have a go at formulation in BRMS

From the last section it was apparent that cmdstanr was insufficiently compatible with tidybayes an possibly other packages.

I will provide the BRMS implementation prototype here for completeness, but I think it will be more useful in future when the tree families can be used in conjunction with other families in the same model e.g. a tree for data infilling, and then a non-linear GAM for frequency/severity models. This capability would be invaluable as the BRMS ability to create stancode for several prototype models relatively quickly is the best method in my opinion.


## Basic install strings

first we would normally need to install development versions of the cmdstanr and BRMS packages. This is already incorporated into the dockerfile.

```{r}
#library(devtools)
#remove.packages(c("StanHeaders", "rstan"))
##install_github("hsbadr/rstan/StanHeaders@develop")
#install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
#install.packages("RcppEigen", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
#install_version("RcppEigen", "0.3.3.9.1")
##install_github("hsbadr/rstan/rstan/rstan@develop")
#install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
#install_github("paul-buerkner/brms",dependencies = TRUE)
```

Make some BRMS stancode for the model. This is a relatively complex model approach but it is useful for adding to other models later.

```{r}

library(brms)
gumbeltree <- custom_family(
  "gumbeltree", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called sigma,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),

  type = "real",
  vars = c("y_T","X_T","H_T","C_T","mu_T","K_T","L_T","NW_T","NL_T","T_T","grainsize_T") # the additional parameters to run the tree
  #vars = c("grainsize_T")
)

stan_funs_gumbeltree <- "
real partial_sum(real[,] xy_slice,
                   int start,
                   int end,
                   matrix H,
                   vector C,
                   vector mu,
                   real sigma,
                   int K,
                   int L,
                   int NW,
                   int NL,
                   real T) {

  real y[end-start+1] = to_array_1d(xy_slice[,1]);
  matrix[end-start+1 , K] X = to_matrix(xy_slice[,2:(K+1)]);
  matrix[NW,end-start+1] w; //calculation of the weight for each node for each sample
  matrix[NL,end-start+1] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,end-start+1)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,end-start+1) ;
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return normal_lpdf(y | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }


  real gumbeltree_lpdf(real dummy_sample // at the moment there is no data that BRMS is handling
                  , real sigma
                  , vector y
                  , matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , real T
                  , int grainsize) {
  // we use this function when the reduce sum is our target. This may not be easy right now with BRMS
  real F;
  F  = reduce_sum(partial_sum,
                       to_array_2d(append_col(y,X)),
                       grainsize,
                       H,
                       C,
                       mu,
                       sigma,
                       K,
                       L,
                       NW,
                       NL,
                       T);
  return F;  
  }

real gumbeltree_noreduce_lpdf(vector y
                  , real sigma
                  , matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , real T
                  , int grainsize) {
  // we use this version of the function when using rstan backend in BRMS which can't yet use reduce_sum
  int N = num_elements(y);
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return normal_lpdf(y | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }

"
```

In the end, we only want to make use of the partial_sum function. So lets simplify our code a little.

```{r}
library(dplyr)
#lets simplify X
  X = data.frame(x1=rep(100,1000)#column of same high number, this is the feature that does nothing
               ,x2=rnorm(1000,0,1)#normalised features 2-10
               ,x3=rnorm(1000,0,1)
               #,x4=rnorm(1000,0,1)
               #,x5=rnorm(1000,0,1)
               #,x6=rnorm(1000,0,1)
               #,x7=rnorm(1000,0,1)
               #,x8=rnorm(1000,0,1)
               #,x9=rnorm(1000,0,1)
               #,x10=rnorm(1000,0,1)
               )%>%
  mutate(y = sin(x2))# a function to try to approximate

```


```{r}
stan_funs_part_sum <- "
real partial_sum(real[,] xy_slice,
                   int start,
                   int end,
                   matrix H,
                   vector C,
                   vector mu,
                   real sigma,
                   int K,
                   int L,
                   int NW,
                   int NL,
                   real T) {

  real y[end-start+1] = to_array_1d(xy_slice[,1]);
  matrix[end-start+1 , K] X = to_matrix(xy_slice[,2:(K+1)]);
  matrix[NW,end-start+1] w; //calculation of the weight for each node for each sample
  matrix[NL,end-start+1] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,end-start+1)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,end-start+1) ;
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return normal_lpdf(y | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }
"

stanvars_trf_data = " 
int NL=1;// number of possible leaves
int NW;// number of nodes or weights or choices in the tree

  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }

  NW=NL-1;
"
stanvars_par = "
  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node
  simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex

  vector[NW] C; //for each feature selected we need a cutoff point
  vector[NL] mu_T;// mean of Y at each leaf
  real<lower=0> sigma_T;// residual error

"

 stanvars_trf_p = "
 matrix[NW,K_T] H; // one hot to select a feature at each node
 
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1)
  }
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree <- stanvar(scode = stan_funs_part_sum, block = "functions")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = stanvars_par, block = "parameters")+
                           stanvar(scode = stanvars_trf_p,block = "tparameters")+
                           stanvar(length(X$y), name = "N_T",scode = "  int N_T;")+ # number of samples
                           stanvar(as.integer(length(X[1,])-1), name = "K_T",scode = "  int K_T;")+ # number of features (minus 1 to exclude y)
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N_T,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(X$y, name = "y_T",scode = "  vector[N_T] y_T;")+# here we add the target samples y directly to the model
                           
                           stanvar(as.integer(4), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(1, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(0.2, name = "alpha_T",scode = "  real alpha_T;")+ # alpha parameter fo the dirichlet prior for P
                           stanvar(length(X$y)%/%10, name = "grainsize_T",scode = "  int grainsize_T;") #suggested grainsize for the reduce_sum for the tree
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           ,prior("target += reduce_sum(partial_sum, to_array_2d( append_col(y_T,X_T)), grainsize_T,H,C, mu_T, sigma_T,K_T,L_T,NW,NL,T_T); // the main LogLikelihood calculation, parallelised via reduce_sum", check = FALSE)
           )
# 
```

### compile stancode

now that I have the components to perform a BRMS setup with the model. lets call BRMS and ask for the code to see it looks right to a superficial check. This is a debugging step where I modify the code above until the stancode looks right.



```{r eval=FALSE, include=FALSE}

 stancode =make_stancode(y~1
                               #, family = gumbeltree # needed if you want to do something else with the data as well as the tree
                               , stanvars = stanvars_gumbeltree
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,iter = 200
                               ,warmup = 200
                               ,chains = 2
                               ,refresh =10
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors
) 

stancode # type out the stancode for bug checking before we try to run it

```


Now that we have created BRMS compatible stancode. Lets try to compile it. This is the next debugging step. We modify the stancode further to get rid of the final issues in the code such that it compiles and runs without error.

```{r eval=FALSE, include=FALSE}

 Test_BRMS =brm(y~1
                               #, family = gumbeltree # needed if you want to do something else with the data as well as the tree
                               , stanvars = stanvars_gumbeltree
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,cores = 20
                               ,iter = 10
                               ,warmup = 10
                               ,chains = 1
                               ,refresh =1
                               ,max_treedepth=10
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors
) 

```

Readers may note that the BRMS package has effectively been used like a trojan horse to run this code. This shows how much control there is over the BRMS package behaviour if you know how stancode works. A real testament to the work Paul Buerkner and his team has put into the development.

The test has worked so we may now run a longer model that may be amenable to further residuals analysis.

```{r eval=FALSE, include=FALSE}
BRMS_example  =brm(y~1
                               #, family = gumbeltree # needed if you want to do something else with the data as well as the tree
                               , stanvars = stanvars_gumbeltree
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,cores = 20
                               ,iter = 200
                               ,warmup = 150
                               ,chains = 4
                               ,refresh =10
                               ,max_treedepth=10
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors
) 

save(BRMS_example, file = "BRMS_example.RData")
```
Completion time isn't great, but the model seemed to work ok. Lets have a look with Shinystan. A bit easier now that the model is in BRMS.

```{r eval=FALSE, include=FALSE}
library(shinystan)
launch_shinystan(BRMS_example)

```
What is evident from ShinyStan is that there is multimodality in the answers and the given means at each leaf. As a result, we need do one of three things:
- simplify the model to understand the outputs better and it runs faster
- Look at the residuals against the sampled data to see if the model is doing any good
- Extract some of the trees to see if they are making decisions in the way we think.

We will start with residuals as this was why we used BRMS in the first place.

As an afterthought. I think I will further modify BRMS to include a new family like I was originally planning

```{r}

library(brms)
gumbeltree <- custom_family(
  "gumbeltree", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called sigma,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),

  type = "real",
  vars = c("y_T","X_T","H","C","mu_T","K_T","L_T","NW","NL","T_T","grainsize_T") # the additional parameters to run the tree
  #vars = c("grainsize_T")
)

stan_funs_gumbeltree <- "
real partial_sum(real[,] xy_slice,
                   int start,
                   int end,
                   matrix H,
                   vector C,
                   vector mu,
                   real sigma,
                   int K,
                   int L,
                   int NW,
                   int NL,
                   real T) {

  real y[end-start+1] = to_array_1d(xy_slice[,1]);
  matrix[end-start+1 , K] X = to_matrix(xy_slice[,2:(K+1)]);
  matrix[NW,end-start+1] w; //calculation of the weight for each node for each sample
  matrix[NL,end-start+1] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,end-start+1)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,end-start+1) ;
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return normal_lpdf(y | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }


  real gumbeltree_lpdf(real dummy_sample // at the moment there is no data that BRMS is handling
                  , real sigma
                  , vector y
                  , matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , real T
                  , int grainsize) {
  // we use this function when the reduce sum is our target. This may not be easy right now with BRMS
  real F;
  F  = reduce_sum(partial_sum,
                       to_array_2d(append_col(y,X)),
                       grainsize,
                       H,
                       C,
                       mu,
                       sigma,
                       K,
                       L,
                       NW,
                       NL,
                       T);
  return F;  
  }

"
stanvars_trf_data = " 
  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
"
stanvars_par = "
  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node
  simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex

"

 stanvars_trf_p = "
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1)
  }
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree <- stanvar(scode = stan_funs_gumbeltree, block = "functions")+
                           stanvar(pll_args = "int NL", name = "NL",scode = "int NL;// # leaves",  block = "tdata")+
                           stanvar(pll_args = "int NW", name = "NW",scode = "int NW;// # splits",  block = "tdata")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = "vector[NW] C; // one hot to select a feature at each node", pll_args = "vector C" ,block = "parameters")+
                           stanvar(scode = "vector[NL] mu_T;// mean of Y at each leaf", pll_args = "vector mu_T" ,block = "parameters")+
                           stanvar(scode = stanvars_par, block = "parameters")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node", pll_args = "matrix H" ,block = "model")+
                           stanvar(scode = stanvars_trf_p, block = "model")+
                           stanvar(length(X$y), name = "N_T",scode = "  int N_T;")+ # number of samples
                           stanvar(as.integer(length(X[1,])-1), name = "K_T",scode = "  int K_T;")+ # number of features (minus 1 to exclude y)
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N_T,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(X$y, name = "y_T",scode = "  vector[N_T] y_T;")+# here we add the target samples y directly to the model
                           
                           stanvar(as.integer(4), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(1, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(0.2, name = "alpha_T",scode = "  real alpha_T;")+ # alpha parameter fo the dirichlet prior for P
                           stanvar(length(X$y)%/%10, name = "grainsize_T",pll_args = "int grainsize_T",scode = "  int grainsize_T;") #suggested grainsize for the reduce_sum for the tree
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           )
# 



```

try out the code generator

```{r eval=FALSE, include=FALSE}
 formula = bf(y ~  1)
 
 stancode =make_stancode(formula
                               , family = gumbeltree # needed if you want to do something else with the data as well as the tree
                               , stanvars = stanvars_gumbeltree
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr" 
                               ,threads = threading(20)
                               ,iter = 200
                               ,warmup = 200
                               ,chains = 2
                               ,refresh =10
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors
) 

stancode # type out the stancode for bug checking before we try to run it

```

now try to compile

```{r eval=FALSE, include=FALSE}
BRMS_example2 =brm(formula
                   , family = gumbeltree
                  ,stanvars = stanvars_gumbeltree
                  ,data = data.frame(y=c(1,2)) 
                  ,backend = "cmdstanr"
                  ,threads = threading(20)
                  ,cores = 20
                  ,iter = 200
                  ,warmup = 150
                  ,chains = 4
                  ,refresh =10
                  ,max_treedepth=10
                  ,adapt_delta =0.9
                  #,inits = "0"
                  #,thin =1
                  ,prior = priors
) 

save(BRMS_example2, file = "BRMS_example2.RData")
```
this also runs. But what if I want the model to store the estimate of y for each row of X. I should have a transformed vector parameter mu_T that is the calculation from the tree.

```{r}

library(brms)
gumbeltree_mu_X <- custom_family(
  "gumbeltree_mu_X", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called mu,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),# sigma needs a log link function to stay positive

  type = "real",
  vars = c("mu_X[n]") # estimates mean for a given X[i] is passed to the function
  #vars = c("grainsize_T")
)
library(brms)

# rewrite the code such that the mu_T is calculated in the transformed parameters section getting rid of H

stan_funs_gumbeltree_mu_X <- "

vector mu_X_calc(matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , int N
                  ) {
  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return temp[1]';// this is mu_X, the estimate of Y given the tree 
}

real gumbeltree_mu_X_lpdf(real y, real sigma, real mu){
  return normal_lpdf(y|mu,sigma);// correctly identify mu parameter as sigma, mu_X as mu
}

"
stanvars_trf_data = " 
  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
"

 stanvars_likelihood = "
  mu_X[start:end] = mu_X_calc( X_T[start:end] ,H,C,mu_T,K_T,L_T,NW,NL,N );
  
 "
 
 stanvars_model = "
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency
  }
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree_mu_X <- stanvar(scode = stan_funs_gumbeltree_mu_X, block = "functions")+
                           stanvar(pll_args = "int NL", name = "NL",scode = "int NL;// # leaves",  block = "tdata")+
                           stanvar(pll_args = "int NW", name = "NW",scode = "int NW;// # splits",  block = "tdata")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = "vector[NW] C; // one hot to select a feature at each node", pll_args = "vector C",block = "parameters")+
                           stanvar(scode = "  vector[NL] mu_T;// mean of Y at each leaf", pll_args = "vector mu_T", block = "parameters")+
                           stanvar(scode = "  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node", block = "parameters")+
                           stanvar(scode = "simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex", block = "parameters")+
                           stanvar(scode = "vector[end] mu_X;" , name = "mu_X",block = "likelihood")+
                           stanvar(scode = stanvars_likelihood, block = "likelihood")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node", pll_args = "matrix H", block = "model")+
                           stanvar(scode = stanvars_model, block = "model")+
                           stanvar(as.integer(length(X[1,])-1), name = "K_T",scode = "  int K_T;")+ # number of features in X_T
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(as.integer(4), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(1, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(0.2, name = "alpha_T",scode = "  real alpha_T;") # alpha parameter fo the dirichlet prior for P
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors_mu_X = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           )
# 



```

write out the new code

```{r eval=FALSE, include=FALSE}
 formula = bf(y ~  1)
 
 stancode_mu_X =make_stancode(formula
                               ,family = gumbeltree_mu_X # needed if you want to do something else with the data as well as the tree
                               ,stanvars = stanvars_gumbeltree_mu_X
                               ,data = (X %>% select(y))
                               #,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,iter = 20
                               ,warmup = 10
                               ,chains = 2
                               ,refresh =1
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors_mu_X
) 

stancode_mu_X # type out the stancode for bug checking before we try to run it

```
ok. now try to compile.

```{r eval=FALSE, include=FALSE}
options("cmdstanr_verbose"=FALSE)

BRMS_mu_X =brm(formula
                ,family = gumbeltree_mu_X # needed if you want to do something else with the data as well as the tree
                ,stanvars = stanvars_gumbeltree_mu_X
                ,data = (X %>% select(y))
                ,backend = "cmdstanr"
                ,threads = threading(20)
                ,cores =20
                ,iter = 200
                ,warmup = 150
                ,chains = 4
                ,refresh =10
                ,max_treedepth=10
                ,adapt_delta =0.9
                #,inits = "0"
                #,thin =1
                ,prior = priors_mu_X
                #,quiet = TRUE
               , silent = 2
) 



save(BRMS_mu_X, file = "BRMS_mu_X.RData")
```
now we can calculate residuals using the calc_mu_X function. Normally expose_functions would do this for us but we used cmdstanr

```{r eval=FALSE, include=FALSE}

# expose_functions(BRMS_mu_X) fails because we are not using rstan
# lets recompile the model with chains=0 to build an rstan version

     BRMS_mu_X_rstan =brm(formula
                     ,family = gumbeltree_mu_X # needed if you want to do something else with the data as well as the tree
                     ,stanvars = stanvars_gumbeltree_mu_X
                     ,data = data.frame(y=1)
                     #,backend = "cmdstanr" commented out so that it is run with rstan
                     #,threads = threading(20)
                     #,cores =20
                     ,iter = 10
                     ,warmup = 5
                     ,chains = 0 # don't run the model, just compile
                     ,refresh =1
                     ,max_treedepth=15
                     ,adapt_delta =0.9
                     #,inits = "0"
                     #,thin =1
                     ,prior = priors_mu_X
                     #,quiet = TRUE
                    ,silent = 0
     ) 

```

This didn't work because rstan runs out of memory trying to compile. So, we will reproduce the function in r instead.
```{r}
# possibly need a function to look at H first

H_calc = function(G,P,NW,K,Temp){

  H = matrix(nrow =NW,ncol = K)# one hot to select a feature at each node
  
 for (j in 1:NW){
      H[j,] = exp( (log(P[j,])*(Temp^2 + Temp +1) / (Temp+1) + G[j,])/Temp) 
      H[j,] = H[j,]/sum(H[j,])
      
      # selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency
 }

  H
}



 mu_X_calc = function( X
                  ,  P
                  ,  G
                  ,  C
                  ,  mu
                  ,  K
                  ,  L
                  ,  NW
                  ,  NL
                  ,  N
                  ,  Temp
                  ) {
   
  H = H_calc(matrix(G[,], nrow = NW)
               ,matrix(P[,], nrow = NW)
               ,NW,K,Temp)
  
  
  # we use this version of the function when calculating mu_T in transformed parameters so
  # that we don't have to declare all the additional variables in trf param section as well
 
  
 # this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit_scaled(10*(H %*% t(X)+ matrix(rep(C,N),ncol=N)))

  # start at the bottom of the tree and work upwards
  # at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = matrix(rep(mu,N),ncol = N) 
  
  for (j in L:1){
   for (k in 1:(2^(j-1))){
     temp[k,]=w[k+NW-2^j+1,] * temp[2*k-1,] + (1-w[k+NW-2^j+1,]) * temp[2*k,] # cycling through the tree and calculating which leaf is retained based on w
   }
  }

  temp[1,] # this is mu_X, the estimate of Y given the tree 
 }
```

Now try to use the function to test it.

```{r }
library(tidybayes)
library(brms)
library(dplyr)
load("BRMS_mu_X.RData")
L = BRMS_mu_X$stanvars$L_T$sdata
K = BRMS_mu_X$stanvars$K_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(BRMS_mu_X)
P = base::array((BRMS_mu_X%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((BRMS_mu_X%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((BRMS_mu_X%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((BRMS_mu_X%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))
N = length(BRMS_mu_X$data$y)
Temp = BRMS_mu_X$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%dplyr::select(-y))
          ,  as.matrix(P[,,1])
          ,  as.matrix(G[,,1])
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```
Ok. now produce residuals for each iteration of the model

```{r}
mu_X = array(dim = c(N,draws))

for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  as.matrix(P[,,i])
          ,  as.matrix(G[,,i])
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  
}
residuals = mu_X-X$y
plot(x=X$x2, y=X$y)
lines(x=X$x2, y=mu_X[,1], type = "p")
lines(x=X$x2, y=mu_X[,10], type = "p")
lines(x=X$x2, y=mu_X[,100], type = "p")
lines(x=X$x2, y=mu_X[,120], type = "p")
lines(x=X$x2, y=mu_X[,140], type = "p")
lines(x=X$x2, y=mu_X[,160], type = "p")
lines(x=X$x2, y=mu_X[,180], type = "p")

```
A terrible failure...

Lets modify the model a little.

We will try a separate C for every feature at every node, and use the feature selection one-hot to select a specific C as well.

Lets also simplify X to just 2 numbers with some error and reduce the tree to a stump. Something is fishy.

```{r}
#minimum simple X, constructed using the same method as the tree tries to fit.

test_sigma = 0.01 # error component
test_C = 0 # cutoff for step function

  X = data.frame(#x1=rep(100,1000) # column of same high number, this is the feature that does nothing. Removed at the moment
               x1=rnorm(1000,0,1) #normalised features 2-10
               #,x3=rnorm(1000,0,1)
               #,x4=rnorm(1000,0,1)
               #,x5=rnorm(1000,0,1)
               #,x6=rnorm(1000,0,1)
               #,x7=rnorm(1000,0,1)
               #,x8=rnorm(1000,0,1)
               #,x9=rnorm(1000,0,1)
               #,x10=rnorm(1000,0,1)
               )%>%
  mutate(y = rnorm(1000,0,test_sigma)+inv_logit_scaled(10*(x1+ test_C)))# a function to try to approximate

  plot(x=X$x1,y=X$y)# ploy y=f(x)
  
```
This is exactly the model that should be soluble by a tree with L=1, K=1, mu1=0 and mu2=1. Lets try to generate these results from 

```{r}
library(brms)

# define tree parameters
T_model = 0.1 # the temperature of the gumbel softmax. 0 ~ one-hot, inf. ~ uniform. T=1 is a good starting point
alpha_model = 0.2 # temperature of the dirichlet. 
L_model = 1 # number of levels in the tree


gumbeltree_mu_X <- custom_family(
  "gumbeltree_mu_X", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called mu,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),# sigma needs a log link function to stay positive

  type = "real",
  vars = c("mu_X[n]") # estimates mean for a given X[i] is passed to the function
  #vars = c("grainsize_T")
)

# rewrite the code such that the mu_T is calculated in the transformed parameters section getting rid of H

stan_funs_gumbeltree_mu_X <- "

vector mu_X_calc(matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , int N
                  ) {
  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));// selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }
    return temp[1]';// this is mu_X, the estimate of Y given the tree 
}

real gumbeltree_mu_X_lpdf(real y, real sigma, real mu){
  return normal_lpdf(y|mu,0.1);// correctly identify mu parameter as sigma, mu_X as mu
}

"
stanvars_trf_data = " 
  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
"

 stanvars_likelihood = "
  mu_X[start:end] = mu_X_calc( X_T[start:end] ,H,Ci,mu_T,K_T,L_T,NW,NL,N );// modified Ci
  //if (start==1){print(mu_X[1]);}
 "
 
 stanvars_model = "
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;// this is the one-hot vector at each node
      
      Ci[j] = H[j]*C[j]'; // selects the right cutoff for the feature chosen at each node
  }
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree_multi_C <- stanvar(scode = stan_funs_gumbeltree_mu_X, block = "functions")+
                           stanvar(pll_args = "int NL", name = "NL",scode = "int NL;// # leaves",  block = "tdata")+
                           stanvar(pll_args = "int NW", name = "NW",scode = "int NW;// # splits",  block = "tdata")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = "matrix[NW,K_T] C; // a cutoff for each feature at each node", pll_args = "matrix C",block = "parameters")+
                           stanvar(scode = "  vector[NL] mu_T;// mean of Y at each leaf", pll_args = "vector mu_T", block = "parameters")+
                           stanvar(scode = "  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node", block = "parameters")+
                           stanvar(scode = "simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex", block = "parameters")+
                           stanvar(scode = "vector[end] mu_X;//expected yi for each Xi" , name = "mu_X",block = "likelihood")+
                           stanvar(scode = stanvars_likelihood, block = "likelihood")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node", pll_args = "matrix H", block = "model")+
                           stanvar(scode = "vector[NW] Ci; // selected cutoff at each node", pll_args = "vector Ci", block = "model")+
                           stanvar(scode = stanvars_model, block = "model")+
                           stanvar(as.integer(length(X%>%select(-y))), name = "K_T",scode = "  int K_T;")+ # number of features in X_T
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(as.integer(L_model), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(T_model, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(alpha_model, name = "alpha_T",scode = "  real alpha_T;") # alpha parameter fo the dirichlet prior for P
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors_mu_X = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("to_vector(C)~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           ,prior("mu_T~normal(0,1);//mu_T also needs a prior and should be near y", check = FALSE)
           )

```

run the simplest possible tree model.

```{r eval=FALSE, include=FALSE}

BRMS_mu_X_simplest =brm(y~1
                ,family = gumbeltree_mu_X # needed if you want to do something else with the data as well as the tree
                ,stanvars = stanvars_gumbeltree_mu_X
                ,data = (X %>% select(y))
                ,backend = "cmdstanr"
                ,threads = threading(20)
                ,cores =20
                ,iter = 200
                ,warmup = 150
                ,chains = 4
                ,refresh =50
                ,max_treedepth=15
                ,adapt_delta =0.9
                #,inits = "0"
                #,thin =1
                ,prior = priors_mu_X
                #,quiet = TRUE
               , silent = 2
) 



save(BRMS_mu_X_simplest, file = "BRMS_mu_X_simplest.RData")
```

now the residuals:
 first grab the parameters.
 
```{r}
load("BRMS_mu_X_simplest.RData")
model = BRMS_mu_X_simplest

L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))
N = length(model$data$y)
Temp = model$stanvars$T_T$sdata

```

Now try to plot true Y vs fitted Y.

```{r}
mu_X = array(dim = c(N,draws))

for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  
}
residuals = mu_X-X$y
plot(x=X$x1, y=X$y, col="red")
lines(x=X$x1, y=mu_X[,1], type = "p")
lines(x=X$x1, y=mu_X[,10], type = "p")
lines(x=X$x1, y=mu_X[,100], type = "p")
lines(x=X$x1, y=mu_X[,120], type = "p")
lines(x=X$x1, y=mu_X[,140], type = "p")
lines(x=X$x1, y=mu_X[,160], type = "p")
lines(x=X$x1, y=mu_X[,180], type = "p")

```

OK. perfect fit :)

Now increase tree size.

```{r}
## a new sample of X
  X = data.frame(x1=(runif(1000)-0.5)*4#column of same high number, this is the feature that does nothing
               #,x2=(runif(1000)-0.5)*4#normalised features 2-10
               #,x3=(runif(1000)-0.5)*4
               #,x4=rnorm(1000,0,1)
               #,x5=rnorm(1000,0,1)
               #,x6=rnorm(1000,0,1)
               #,x7=rnorm(1000,0,1)
               #,x8=rnorm(1000,0,1)
               #,x9=rnorm(1000,0,1)
               #,x10=rnorm(1000,0,1)
               )%>%
  mutate(y = sin(x1))# a function to try to approximate

```

found a bug in my code for tree calculations.
```{r}
library(brms)

# define tree parameters
T_model = 0.1 # the temperature of the gumbel softmax. 0 ~ one-hot, inf. ~ uniform. T=1 is a good starting point
alpha_model = 0.2 # temperature of the dirichlet. 
L_model = 2 # number of levels in the tree


gumbeltree_mu_X <- custom_family(
  "gumbeltree_mu_X", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called mu,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),# sigma needs a log link function to stay positive

  type = "real",
  vars = c("mu_X[n]") # estimates mean for a given X[i] is passed to the function
  #vars = c("grainsize_T")
)

# rewrite the code such that the mu_T is calculated in the transformed parameters section getting rid of H

stan_funs_gumbeltree_mu_X <- "

vector mu_X_calc(matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , int N
                  ) {
  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));// selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in L:1){
   c2=1;
   for (k in 1:j){
     c2 = c2*2;// can only make integer c2 = (2^j) inside a loop
   }
   c = c2/2;
   for (k in 1:c){
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }
    return temp[1]';// this is mu_X, the estimate of Y given the tree 
}

real gumbeltree_mu_X_lpdf(real y, real sigma, real mu){
  return normal_lpdf(y|mu,0.1);// correctly identify mu parameter as sigma, mu_X as mu
}

"
stanvars_trf_data = " 
  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
"

 stanvars_likelihood = "
  mu_X[start:end] = mu_X_calc( X_T[start:end] ,H,Ci,mu_T,K_T,L_T,NW,NL,N );// modified Ci
  //if (start==1){print(mu_X[1]);}
 "
 
 stanvars_model = "
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;// this is the one-hot row-vector at each node
      
      Ci[j] = H[j]*C[j]'; // selects the right cutoff for the feature chosen at each node
  }
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree_multi_C <- stanvar(scode = stan_funs_gumbeltree_mu_X, block = "functions")+
                           stanvar(pll_args = "int NL", name = "NL",scode = "int NL;// # leaves",  block = "tdata")+
                           stanvar(pll_args = "int NW", name = "NW",scode = "int NW;// # splits",  block = "tdata")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = "matrix<lower=-2, upper=2>[NW,K_T] C; // a cutoff for each feature at each node", pll_args = "matrix C",block = "parameters")+
                           stanvar(scode = "  vector[NL] mu_T;// mean of Y at each leaf", pll_args = "vector mu_T", block = "parameters")+
                           stanvar(scode = "  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node", block = "parameters")+
                           stanvar(scode = "simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex", block = "parameters")+
                           stanvar(scode = "vector[end] mu_X;//expected yi for each Xi" , name = "mu_X",block = "likelihood")+
                           stanvar(scode = stanvars_likelihood, block = "likelihood")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node", pll_args = "matrix H", block = "model")+
                           stanvar(scode = "vector[NW] Ci; // selected cutoff at each node", pll_args = "vector Ci", block = "model")+
                           stanvar(scode = stanvars_model, block = "model")+
                           stanvar(as.integer(length(X%>%select(-y))), name = "K_T",scode = "  int K_T;")+ # number of features in X_T
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(as.integer(L_model), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(T_model, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(alpha_model, name = "alpha_T",scode = "  real alpha_T;") # alpha parameter fo the dirichlet prior for P
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors_mu_X = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("to_vector(C)~normal(0,3); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           ,prior("mu_T~normal(0,5);//mu_T also needs a prior and should be near y", check = FALSE)
           )

```


```{r eval=FALSE, include=FALSE}
options("cmdstanr_verbose"=FALSE)

BRMS_multi_C =brm(formula
                ,family = gumbeltree_mu_X # needed if you want to do something else with the data as well as the tree
                ,stanvars = stanvars_gumbeltree_multi_C
                ,data = (X %>% select(y))
                ,backend = "cmdstanr"
                ,threads = threading(20)
                ,cores =20
                ,iter = 500
                ,warmup = 150
                ,chains = 4
                ,refresh =50
                ,max_treedepth=18
                ,adapt_delta =0.9
                #,inits = "0"
                #,thin =1
                ,prior = priors_mu_X
                #,quiet = TRUE
               , silent = 2
) 



save(BRMS_multi_C, file = "BRMS_multi_C.RData")
```

```{r}
#load("BRMS_mu_X.Rdata")
 mu_X_calc_multi_C = function( X
                  ,  P
                  ,  G
                  ,  C
                  ,  mu
                  ,  K
                  ,  L
                  ,  NW
                  ,  NL
                  ,  N
                  ,  Temp
                  ) {
   
  H= matrix(nrow =NW,ncol = K)# one hot to select a feature at each node
  Ci = vector()
  H = H_calc(G,P,NW,K,Temp)
 for (j in 1:NW){
      Ci[j] = H[j,] %*% as.vector(C[j,])
      # selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency
 }
  
  
  # we use this version of the function when calculating mu_T in transformed parameters so
  # that we don't have to declare all the additional variables in trf param section as well
  
  
 # this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit_scaled(10*(H %*% t(X)+ matrix(rep(Ci,N),ncol=N)))

  # start at the bottom of the tree and work upwards
  # at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = matrix(rep(mu,N),ncol = N) 
  
  for (j in L:1){
   for (k in 1:(2^(j-1))){
     temp[k,]=w[k+NW-2^j+1,] * temp[2*k-1,] + (1-w[k+NW-2^j+1,]) * temp[2*k,] # cycling through the tree and calculating which leaf is retained based on w
   }
  }

  temp[1,] # this is mu_X, the estimate of Y given the tree 
 }
```

```{r}
library(tidybayes)
load("BRMS_multi_C.RData")

model = BRMS_multi_C
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$C, dim = c(NW,K,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))
N = length(model$data$y)
Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc_multi_C( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  matrix(C[,,1], nrow = NW)
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc_multi_C( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  matrix(C[,,i], nrow = NW)
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}
residuals = mu_X-X$y

plot(x=X$x1, y=X$y, col="red")
plot(x=X$x1, y=mu_X[,1], type = "p")
lines(x=X$x1, y=mu_X[,10], type = "p")
lines(x=X$x1, y=mu_X[,100], type = "p")
lines(x=X$x1, y=mu_X[,120], type = "p")
lines(x=X$x1, y=mu_X[,140], type = "p")
lines(x=X$x1, y=mu_X[,160], type = "p")
lines(x=X$x1, y=mu_X[,180], type = "p")

```
Didn't work. Diagnose by holding C's static. Moving back to single C per node.


```{r}
## a new sample of X
  X = data.frame(x1=(runif(1000)-0.5)*4#column of same high number, this is the feature that does nothing
               #,x2=(runif(1000)-0.5)*4#normalised features 2-10
               #,x3=(runif(1000)-0.5)*4
               #,x4=rnorm(1000,0,1)
               #,x5=rnorm(1000,0,1)
               #,x6=rnorm(1000,0,1)
               #,x7=rnorm(1000,0,1)
               #,x8=rnorm(1000,0,1)
               #,x9=rnorm(1000,0,1)
               #,x10=rnorm(1000,0,1)
               )%>%
  mutate(y = sin(x1))# a function to try to approximate

```

found a bug in my code for tree calculations.
```{r}
library(brms)

# define tree parameters
T_model = 0.1 # the temperature of the gumbel softmax. 0 ~ one-hot, inf. ~ uniform. T=1 is a good starting point
alpha_model = 0.2 # temperature of the dirichlet. 
L_model = 2 # number of levels in the tree


gumbeltree_mu_X <- custom_family(
  "gumbeltree_mu_X", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called mu,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),# sigma needs a log link function to stay positive

  type = "real",
  vars = c("mu_X[n]") # estimates mean for a given X[i] is passed to the function
  #vars = c("grainsize_T")
)

# rewrite the code such that the mu_T is calculated in the transformed parameters section getting rid of H

stan_funs_gumbeltree_mu_X <- "

vector mu_X_calc(matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , int N
                  ) {
  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));// selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in L:1){
   c2=1;
   for (k in 1:j){
     c2 = c2*2;// can only make integer c2 = (2^j) inside a loop
   }
   c = c2/2;
   for (k in 1:c){
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }
    return temp[1]';// this is mu_X, the estimate of Y given the tree 
}

real gumbeltree_mu_X_lpdf(real y, real sigma, real mu){
  return normal_lpdf(y|mu,0.1);// correctly identify mu parameter as sigma, mu_X as mu
}

"
stanvars_trf_data = " 
  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
"

 stanvars_likelihood = "
  mu_X[start:end] = mu_X_calc( X_T[start:end] ,H,C,mu_T,K_T,L_T,NW,NL,N );
  //if (start==1){print(mu_X[1]);}
 "
 
 stanvars_model = "
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;// this is the one-hot row-vector at each node
  }
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree_multi_C <- stanvar(scode = stan_funs_gumbeltree_mu_X, block = "functions")+
                           stanvar(pll_args = "int NL", name = "NL",scode = "int NL;// # leaves",  block = "tdata")+
                           stanvar(pll_args = "int NW", name = "NW",scode = "int NW;// # splits",  block = "tdata")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = "vector<lower=-2, upper=2>[NW] C; // a cutoff for each feature at each node", pll_args = "vector C",block = "parameters")+
                           stanvar(scode = "  vector[NL] mu_T;// mean of Y at each leaf", pll_args = "vector mu_T", block = "parameters")+
                           stanvar(scode = "  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node", block = "parameters")+
                           stanvar(scode = "simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex", block = "parameters")+
                           stanvar(scode = "vector[end] mu_X;//expected yi for each Xi" , name = "mu_X",block = "likelihood")+
                           stanvar(scode = stanvars_likelihood, block = "likelihood")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node", pll_args = "matrix H", block = "model")+
                           stanvar(scode = stanvars_model, block = "model")+
                           stanvar(as.integer(length(X%>%select(-y))), name = "K_T",scode = "  int K_T;")+ # number of features in X_T
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(as.integer(L_model), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(T_model, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(alpha_model, name = "alpha_T",scode = "  real alpha_T;") # alpha parameter fo the dirichlet prior for P
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors_mu_X = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("C[1] ~ normal(-1,0.001);", check = FALSE)
           ,prior("C[2] ~ normal(1,0.001);", check = FALSE)
           ,prior("C[3] ~ normal(0,0.001);", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           ,prior("mu_T~normal(0,5);//mu_T also needs a prior and should be near y", check = FALSE)
           )

```


```{r eval=FALSE, include=FALSE}
options("cmdstanr_verbose"=FALSE)

BRMS_static_C =brm(formula
                ,family = gumbeltree_mu_X # needed if you want to do something else with the data as well as the tree
                ,stanvars = stanvars_gumbeltree_multi_C
                ,data = (X %>% select(y))
                ,backend = "cmdstanr"
                ,threads = threading(20)
                ,cores =20
                ,iter = 500
                ,warmup = 150
                ,chains = 4
                ,refresh =50
                ,max_treedepth=18
                ,adapt_delta =0.9
                #,inits = "0"
                #,thin =1
                ,prior = priors_mu_X
                #,quiet = TRUE
               , silent = 2
) 



save(BRMS_static_C, file = "BRMS_static_C.RData")
```


```{r}
library(tidybayes)

load("BRMS_static_C.RData")
model = BRMS_static_C
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))
N = length(model$data$y)
Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}
residuals = mu_X-X$y
```

```{r}
#some plots
plot(x=X$x1, y=X$y, col="red", ylim=c(-5,5))
lines(x=X$x1, y=mu_X[,2], type = "p")
lines(x=X$x1, y=mu_X[,3], type = "p")
lines(x=X$x1, y=mu_X[,4], type = "p")
lines(x=X$x1, y=mu_X[,5], type = "p")
lines(x=X$x1, y=mu_X[,6], type = "p")
lines(x=X$x1, y=mu_X[,7], type = "p")
lines(x=X$x1, y=mu_X[,8], type = "p")

```
not working properly if you ask me (pretty funny actually). lets check a synthetic model again. this time with 2 layers.



```{r}
## a new sample of X
  X = data.frame(x1=(runif(1000)-0.5)*4#column of same high number, this is the feature that does nothing
               #,x2=(runif(1000)-0.5)*4#normalised features 2-10
               #,x3=(runif(1000)-0.5)*4
               #,x4=rnorm(1000,0,1)
               #,x5=rnorm(1000,0,1)
               #,x6=rnorm(1000,0,1)
               #,x7=rnorm(1000,0,1)
               #,x8=rnorm(1000,0,1)
               #,x9=rnorm(1000,0,1)
               #,x10=rnorm(1000,0,1)
               )
y = mu_X_calc( as.matrix(X)
          ,  matrix(c(1,1,1), nrow = NW)
          ,  matrix(c(1,1,1), nrow = NW)
          ,  as.vector(c(-1,1,0))
          ,  as.vector(c(-1,-0.5,0.25,0.75))
          ,  1#K
          ,  2# L
          ,  3# NW
          ,  4# NL
          ,  1000 # N
          ,  1 # gumbel softmax temp
          )
X = X%>%mutate(y=y)
plot(X$x1,X$y) #show the function as a plot

```
Generating a model that uses only one dimension of x, and is generated with the same model that we are fitting. You can see gentle rounded steps here rather than the sharp steps an actual tree process would generate. Four leaves at four levels of y are needed at four ranges on x1.

```{r}
library(brms)

# define tree parameters
T_model = 0.1 # the temperature of the gumbel softmax. 0 ~ one-hot, inf. ~ uniform. T=1 is a good starting point
alpha_model = 0.2 # temperature of the dirichlet. 
L_model = 2 # number of levels in the tree


gumbeltree_mu_X <- custom_family(
  "gumbeltree_mu_X", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called mu,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),# sigma needs a log link function to stay positive

  type = "real",
  vars = c("mu_X[n]") # estimates mean for a given X[i] is passed to the function
  #vars = c("grainsize_T")
)

# rewrite the code such that the mu_T is calculated in the transformed parameters section getting rid of H

stan_funs_gumbeltree_mu_X <- "

vector mu_X_calc(matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , int N
                  ) {
  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));// selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in L:1){
   c2=1;
   for (k in 1:j){
     c2 = c2*2;// can only make integer c2 = (2^j) inside a loop
   }
   c = c2/2;
   for (k in 1:c){
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }
    return temp[1]';// this is mu_X, the estimate of Y given the tree 
}

real gumbeltree_mu_X_lpdf(real y, real sigma, real mu){
  return normal_lpdf(y|mu,0.1);// correctly identify mu parameter as sigma, mu_X as mu
}

"
stanvars_trf_data = " 
  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
"

 stanvars_likelihood = "
  mu_X[start:end] = mu_X_calc( X_T[start:end] ,H,C,mu_T,K_T,L_T,NW,NL,N );
  //if (start==1){print(mu_X[1]);}
 "
 
 stanvars_model = "
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;// this is the one-hot row-vector at each node
  }
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree_multi_C <- stanvar(scode = stan_funs_gumbeltree_mu_X, block = "functions")+
                           stanvar(pll_args = "int NL", name = "NL",scode = "int NL;// # leaves",  block = "tdata")+
                           stanvar(pll_args = "int NW", name = "NW",scode = "int NW;// # splits",  block = "tdata")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = "vector<lower=-2, upper=2>[NW] C; // a cutoff for each feature at each node", pll_args = "vector C",block = "parameters")+
                           stanvar(scode = "  vector[NL] mu_T;// mean of Y at each leaf", pll_args = "vector mu_T", block = "parameters")+
                           stanvar(scode = "  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node", block = "parameters")+
                           stanvar(scode = "simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex", block = "parameters")+
                           stanvar(scode = "vector[end] mu_X;//expected yi for each Xi" , name = "mu_X",block = "likelihood")+
                           stanvar(scode = stanvars_likelihood, block = "likelihood")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node", pll_args = "matrix H", block = "model")+
                           stanvar(scode = stanvars_model, block = "model")+
                           stanvar(as.integer(length(X%>%select(-y))), name = "K_T",scode = "  int K_T;")+ # number of features in X_T
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(as.integer(L_model), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(T_model, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(alpha_model, name = "alpha_T",scode = "  real alpha_T;") # alpha parameter fo the dirichlet prior for P
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors_mu_X = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("C[1] ~ normal(-1,0.001);", check = FALSE)
           ,prior("C[2] ~ normal(1,0.001);", check = FALSE)
           ,prior("C[3] ~ normal(0,0.001);", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           ,prior("mu_T~normal(0,5);//mu_T also needs a prior and should be near y", check = FALSE)
           )

```


```{r eval=FALSE, include=FALSE}
options("cmdstanr_verbose"=FALSE)

BRMS_2L_sim =brm(formula
                ,family = gumbeltree_mu_X # needed if you want to do something else with the data as well as the tree
                ,stanvars = stanvars_gumbeltree_multi_C
                ,data = (X %>% select(y))
                ,backend = "cmdstanr"
                ,threads = threading(20)
                ,cores =20
                ,iter = 500
                ,warmup = 150
                ,chains = 4
                ,refresh =50
                ,max_treedepth=18
                ,adapt_delta =0.9
                #,inits = "0"
                #,thin =1
                ,prior = priors_mu_X
                #,quiet = TRUE
               , silent = 2
) 



save(BRMS_2L_sim, file = "BRMS_2L_sim.RData")
```


```{r}
library(tidybayes)

load("BRMS_2L_sim.RData")
model = BRMS_2L_sim
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))
N = length(model$data$y)
Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}
residuals = mu_X-X$y
```

```{r}
#some plots
plot(x=X$x1, y=X$y, col="red", ylim=c(-5,5))
lines(x=X$x1, y=mu_X[,502], type = "p")
lines(x=X$x1, y=mu_X[,503], type = "p")
lines(x=X$x1, y=mu_X[,504], type = "p")
lines(x=X$x1, y=mu_X[,505], type = "p")
lines(x=X$x1, y=mu_X[,506], type = "p")
lines(x=X$x1, y=mu_X[,507], type = "p")
lines(x=X$x1, y=mu_X[,508], type = "p")

```
Epic fail. This suggests a bug in the stancode. lets try reducing N to 12, with L=2, K=1, samples evenly spaced on X. We will use the print function to check that the y and x samples match as there might be a bug in the code somewhere.

```{r}
## a new sample of X
  X = data.frame(x1=((1:12)/13-0.5)*4#column of same high number, this is the feature that does nothing
               #,x2=(runif(1000)-0.5)*4#normalised features 2-10
               #,x3=(runif(1000)-0.5)*4
               #,x4=rnorm(1000,0,1)
               #,x5=rnorm(1000,0,1)
               #,x6=rnorm(1000,0,1)
               #,x7=rnorm(1000,0,1)
               #,x8=rnorm(1000,0,1)
               #,x9=rnorm(1000,0,1)
               #,x10=rnorm(1000,0,1)
               )
y = mu_X_calc( as.matrix(X)
          ,  matrix(c(1,1,1), nrow = NW)
          ,  matrix(c(1,1,1), nrow = NW)
          ,  as.vector(c(-1,1,0))
          ,  as.vector(c(-1,-0.5,0.25,0.75))
          ,  1#K
          ,  2# L
          ,  3# NW
          ,  4# NL
          ,  12 # N
          ,  1 # gumbel softmax temp
          )
X = X%>%mutate(y=y)
plot(X$x1,X$y) #show the function as a plot

```


```{r}
library(brms)

# define tree parameters
T_model = 0.1 # the temperature of the gumbel softmax. 0 ~ one-hot, inf. ~ uniform. T=1 is a good starting point
alpha_model = 0.2 # temperature of the dirichlet. 
L_model = 2 # number of levels in the tree


gumbeltree_mu_X <- custom_family(
  "gumbeltree_mu_X", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called mu,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),# sigma needs a log link function to stay positive

  type = "real",
  vars = c("mu_X[n]") # estimates mean for a given X[i] is passed to the function
  #vars = c("grainsize_T")
)

# rewrite the code such that the mu_T is calculated in the transformed parameters section getting rid of H

stan_funs_gumbeltree_mu_X <- "

vector mu_X_calc(matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , int N
                  ) {
  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
for(i in 1:NW){
for(j in 1:N){
  w[i,j] = inv_logit(10*(H[i]*X[j]'+C[i]));
}
}
  //w=inv_logit(10*((H*X')+rep_matrix(C,N)));// selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in L:1){
   c2=1;
   for (k in 1:j){
     c2 = c2*2;// can only make integer c2 = (2^j) inside a loop
   }
   c = c2/2;
   for (k in 1:c){
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }
    return temp[1]';// this is mu_X, the estimate of Y given the tree 
}

real gumbeltree_mu_X_lpdf(real y, real sigma, real mu_X){
  return normal_lpdf(y|mu_X,0.001);// correctly identify mu parameter as sigma, mu_X as mu
}

"
stanvars_trf_data = " 
  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
  print(Y);
"

 stanvars_likelihood = "
  mu_X[start:end] = mu_X_calc( X_T[start:end] ,H,C,mu_T,K_T,L_T,NW,NL,N );
  //if (start==1){print(mu_X[1]);}
 "
 
 stanvars_model = "
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;// this is the one-hot row-vector at each node
 }
 mu_X = mu_X_calc( X_T ,H,C,mu_T,K_T,L_T,NW,NL,N );
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree_multi_C <- stanvar(scode = stan_funs_gumbeltree_mu_X, block = "functions")+
                           stanvar(pll_args = "int NL", name = "NL",scode = "int NL;// # leaves",  block = "tdata")+
                           stanvar(pll_args = "int NW", name = "NW",scode = "int NW;// # splits",  block = "tdata")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = "vector<lower=-2, upper=2>[NW] C; // a cutoff for each feature at each node", pll_args = "vector C",block = "parameters")+
                           stanvar(scode = "  vector<lower=-2, upper=2>[NL] mu_T;// mean of Y at each leaf", pll_args = "vector mu_T", block = "parameters")+
                           stanvar(scode = "  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node", block = "parameters")+
                           stanvar(scode = "simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex", block = "parameters")+
                           stanvar(scode = "vector[N] mu_X;//expected yi for each Xi" , pll_args = "vector mu_X", name = "mu_X",block = "model")+
                           #stanvar(scode = stanvars_likelihood, block = "likelihood")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node", pll_args = "matrix H", block = "model")+
                           stanvar(scode = stanvars_model, block = "model")+
                           stanvar(as.integer(length(X%>%select(-y))), name = "K_T",scode = "  int K_T;")+ # number of features in X_T
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(as.integer(L_model), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(T_model, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(alpha_model, name = "alpha_T",scode = "  real alpha_T;") # alpha parameter fo the dirichlet prior for P
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors_mu_X = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("C[1] ~ normal(-1,0.001);", check = FALSE)
           ,prior("C[2] ~ normal(1,0.001);", check = FALSE)
           ,prior("C[3] ~ normal(0,0.001);", check = FALSE)
           #,prior("mu_T[1] ~ normal(-1,0.001);", check = FALSE)
           #,prior("mu_T[2] ~ normal(-0.5,0.001);", check = FALSE)
           #,prior("mu_T[3] ~ normal(0.25,0.001);", check = FALSE)
           #,prior("mu_T[4] ~ normal(0.75,0.001);", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           #,prior("mu_T~normal(0,5);//mu_T also needs a prior and should be near y", check = FALSE)
           )

```


```{r eval=FALSE, include=FALSE}
options("cmdstanr_verbose"=FALSE)

BRMS_12x =brm(formula
                ,family = gumbeltree_mu_X # needed if you want to do something else with the data as well as the tree
                ,stanvars = stanvars_gumbeltree_multi_C
                ,data = (X %>% select(y))
                ,backend = "cmdstanr"
                ,threads = threading(20)
                ,cores =20
                ,iter = 200
                ,warmup = 150
                ,chains = 1
                ,refresh =50
                ,max_treedepth=10
                ,adapt_delta =0.9
                #,inits = "0"
                #,thin =1
                ,prior = priors_mu_X
                #,quiet = TRUE
               , silent = 2
) 



save(BRMS_12x, file = "BRMS_12x.RData")
```


```{r}
library(tidybayes)

load("BRMS_12x.RData")
model = BRMS_12x
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))
N = length(model$data$y)
Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}

```

```{r}
#some plots


quantiles = array(dim = c(N,3))

for (i in 1:N){
  quantiles[i,] = quantile(mu_X[i,], probs = c(0.01,0.5,0.99))
}
plot(x=X$x1, y=quantiles[,1],  ylim=c(-2,2), col="green")
lines(x=X$x1, y=quantiles[,2], type = "p", col="orange")
lines(x=X$x1, y=quantiles[,3], type = "p", col="blue")
lines(x=X$x1, y=X$y, col="red",type = "p")

```
Found another bug. When y is passed to BRMS, it is re-ordered from max to min. This is the opposite order that I calculated, so the model would be randomising y essentially.

Bug fix is to not pass y as part of the data, or to also pass X so it is also re-ordered. The easy option is to pass my own y_T separately. But I will fall back to the original BRMS implementation I think. After fixing bugs and simplifying X.

I also found the most serious bug. All the mu_X are the same number. Needed to fix the for loop (j in L_T:1). It was never entered for trees larger than L_T=1 because the algorithm only accepts increasing j!!


```{r}

# define tree parameters
T_model = 0.1 # the temperature of the gumbel softmax. 0 ~ one-hot, inf. ~ uniform. T=1 is a good starting point
alpha_model = 0.2 # temperature of the dirichlet. 
L_model = 2 # number of levels in the tree
grainsize_model = 1 # how many rows per chunk to multithread the loglikelihood calculation

stan_funs_part_sum <- "
vector mu_X_calc(matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , int N
                  ) {
  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature

  w=inv_logit(10*((H*X')+rep_matrix(C,N)));// selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in 1:L){ // **** main bug fix. This has to be increasing. Stan does not accept decreasing ranges of j for loops.
   c2=1;
   c=1+L-j; // need c to be the lowest level of the tree working upwards
   for (l in 1:c){
       c2 = c2*2;// can only make integer c2 = 2^C inside a loop
   }
   
   for (k in 1:c2/2){
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }
    return temp[1,]';// this is mu_X, the estimate of Y given the tree 
}

real partial_sum(real[,] xy_slice,
                   int start,
                   int end,
                   matrix H,
                   vector C,
                   vector mu,
                   real sigma,
                   int K,
                   int L,
                   int NW,
                   int NL,
                   real T) {

  real y[end-start+1] = to_array_1d(xy_slice[,1]);
  matrix[end-start+1 , K] X = to_matrix(xy_slice[,2:(K+1)]);
  matrix[NW,end-start+1] w; //calculation of the weight for each node for each sample
  matrix[NL,end-start+1] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,end-start+1)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,end-start+1) ;
  for (j in 1:L){
   c2=1;
   c=1+L-j; // need c to be the lowest level of the tree working upwards
   for (l in 1:c){
       c2 = c2*2;// can only make integer c2 = 2^C inside a loop
   }
   
   for (k in 1:c2/2){
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return normal_lpdf(y | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }
"

stanvars_trf_data = " 
int NL=1;// number of possible leaves
int NW;// number of nodes or weights or choices in the tree

  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }

  NW=NL-1;
"
stanvars_par = "
  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node
  simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex

  vector<lower=-2,upper=2>[NW] C; //for each feature selected we need a cutoff point
  vector<lower=-2,upper=2>[NL] mu_T;// mean of Y at each leaf
  real<lower=0> sigma_T;// residual error

"

 stanvars_model = "
 matrix[NW,K_T] H; // one hot to select a feature at each node
 
  matrix[NW,N] w; 
  matrix[NL,N] temp; 
  int c;// local integer parameter
  int c2;// local integer parameter
 
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1)
 }
 
 
  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X_T')+rep_matrix(C,N_T)));// selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency
  
  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu_T,N_T) ;
  
  for (j in 1:L_T){
   c2=1;
   c=1+L_T-j; // need c to be the lowest level of the tree working upwards
   for (l in 1:c){
       c2 = c2*2;// can only make integer c2 = 2^C inside a loop
   }
   //print(\"j, \",j);
   for (k in 1:c2/2){
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
     //print(\"k, \",k);
   }
  }
  ;   
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree <- stanvar(scode = stan_funs_part_sum, block = "functions")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = stanvars_par, block = "parameters")+
                           stanvar(scode = stanvars_model,block = "model")+
                           stanvar(length(X$y), name = "N_T",scode = "  int N_T;")+ # number of samples
                           stanvar(as.integer(length(X[1,])-1), name = "K_T",scode = "  int K_T;")+ # number of features (minus 1 to exclude y)
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N_T,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(X$y, name = "y_T",scode = "  vector[N_T] y_T;")+# here we add the target samples y directly to the model
                           stanvar(as.integer(L_model), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(T_model, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(alpha_model, name = "alpha_T",scode = "  real alpha_T;")+ # alpha parameter fo the dirichlet prior for P
                           stanvar(grainsize_model, name = "grainsize_T",scode = "  int grainsize_T;") #suggested grainsize for the reduce_sum for the tree
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           #,prior("C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           ,prior("target += reduce_sum(partial_sum, to_array_2d( append_col(y_T,X_T)), grainsize_T,H,C, mu_T, sigma_T,K_T,L_T,NW,NL,T_T); // the main LogLikelihood calculation, parallelised via reduce_sum", check = FALSE)
           ,prior("C[1] ~ normal(-1,0.001);", check = FALSE)
           ,prior("C[2] ~ normal(1,0.001);", check = FALSE)
           ,prior("C[3] ~ normal(0,0.001);", check = FALSE)
           ,prior("mu_T[1] ~ normal(-1,0.001);", check = FALSE)
           ,prior("mu_T[2] ~ normal(-0.5,0.001);", check = FALSE)
           ,prior("mu_T[3] ~ normal(0.25,0.001);", check = FALSE)
           ,prior("mu_T[4] ~ normal(0.75,0.001);", check = FALSE)
           ,prior("sigma_T ~ normal(0.01,0.00001);", check = FALSE)
           )
# 
```

look at the code briefly

```{r}
 formula = bf(y ~  1)


 options("cmdstanr_verbose"=FALSE)
 stancode =make_stancode(formula
                               , stanvars = stanvars_gumbeltree
                               ,sample_prior="only"# ignore the brms model entirely
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,iter = 200
                               ,warmup = 200
                               ,chains = 2
                               ,refresh =10
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors
) 

stancode # type out the stancode for bug checking before we try to run it

```


Lets try to compile it.  

```{r eval=FALSE, include=FALSE}

 Test_BRMS =brm(formula
                               , stanvars = stanvars_gumbeltree
                               ,sample_prior="only"# ignore the brms model entirely
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,cores = 20
                               ,iter = 10
                               ,warmup = 10
                               ,chains = 1
                               ,refresh =1
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors
) 

```



```{r eval=FALSE, include=FALSE}
BRMS_min_data =brm(formula
                               , stanvars = stanvars_gumbeltree
                               ,sample_prior="only"# ignore the brms model entirely
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,cores = 20
                               ,iter = 200
                               ,warmup = 150
                               ,chains = 4
                               ,refresh =50
                               ,max_treedepth=12
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors
) 

save(BRMS_min_data, file = "BRMS_min_data.RData")
```
now for some plots


```{r}
library(tidybayes)

load("BRMS_min_data.RData")
model = BRMS_min_data
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
N = model$stanvars$N_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))

Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}

```

```{r}
#some plots


quantiles = array(dim = c(N,3))

for (i in 1:N){
  quantiles[i,] = quantile(mu_X[i,], probs = c(0.01,0.5,0.99))
}
plot(x=X$x1, y=quantiles[,1],  ylim=c(-2,2), col="green")
lines(x=X$x1, y=quantiles[,2], type = "p", col="orange")
lines(x=X$x1, y=quantiles[,3], type = "p", col="blue")
lines(x=X$x1, y=X$y, col="red",type = "p")

```
OK. working now:) Pernitious bugs!!

Now remove unnecessary code and priors. And upgrade X.

```{r}
N=1000
## a new sample of X
  X = data.frame(x1=(runif(N)-0.5)*4#column of same high number, this is the feature that does nothing
               #,x2=(runif(1000)-0.5)*4#normalised features 2-10
               #,x3=(runif(1000)-0.5)*4
               #,x4=rnorm(1000,0,1)
               #,x5=rnorm(1000,0,1)
               #,x6=rnorm(1000,0,1)
               #,x7=rnorm(1000,0,1)
               #,x8=rnorm(1000,0,1)
               #,x9=rnorm(1000,0,1)
               #,x10=rnorm(1000,0,1)
               )
y = rnorm(N,0,0.05)+mu_X_calc( as.matrix(X)
          ,  matrix(c(1,1,1), nrow = NW)
          ,  matrix(c(1,1,1), nrow = NW)
          ,  as.vector(c(-1,1,0))
          ,  as.vector(c(-1,-0.5,0.25,0.75))
          ,  1#K
          ,  2# L
          ,  3# NW
          ,  4# NL
          ,  1000 # N
          ,  1 # gumbel softmax temp
          )
X = X%>%mutate(y=y)
plot(X$x1,X$y) #show the function as a plot

```

```{r}

# define tree parameters
T_model = 0.1 # the temperature of the gumbel softmax. 0 ~ one-hot, inf. ~ uniform. T=1 is a good starting point
alpha_model = 0.2 # temperature of the dirichlet. 
L_model = 2 # number of levels in the tree
grainsize_model = 1 # how many rows per chunk to multithread the loglikelihood calculation

stan_funs_part_sum <- "

real partial_sum(real[,] xy_slice,
                   int start,
                   int end,
                   matrix H,
                   vector C,
                   vector mu,
                   real sigma,
                   int K,
                   int L,
                   int NW,
                   int NL,
                   real T) {

  real y[end-start+1] = to_array_1d(xy_slice[,1]);
  matrix[end-start+1 , K] X = to_matrix(xy_slice[,2:(K+1)]);
  matrix[NW,end-start+1] w; //calculation of the weight for each node for each sample
  matrix[NL,end-start+1] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,end-start+1)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,end-start+1) ;
  for (j in 1:L){
   c2=1;
   c=1+L-j; // need c to be the lowest level of the tree working upwards
   for (l in 1:c){
       c2 = c2*2;// can only make integer c2 = 2^C inside a loop
   }
   
   for (k in 1:c2/2){
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return normal_lpdf(y | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }
"

stanvars_trf_data = " 
int NL=1;// number of possible leaves
int NW;// number of nodes or weights or choices in the tree

  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }

  NW=NL-1;
"
stanvars_par = "
  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node
  simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex

  vector<lower=-2,upper=2>[NW] C; //for each feature selected we need a cutoff point
  vector<lower=-2,upper=2>[NL] mu_T;// mean of Y at each leaf
  real<lower=0> sigma_T;// residual error

"

 stanvars_model = "
 matrix[NW,K_T] H; // one hot to select a feature at each node
 
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1)
 }
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree <- stanvar(scode = stan_funs_part_sum, block = "functions")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = stanvars_par, block = "parameters")+
                           stanvar(scode = stanvars_model,block = "model")+
                           stanvar(length(X$y), name = "N_T",scode = "  int N_T;")+ # number of samples
                           stanvar(as.integer(length(X[1,])-1), name = "K_T",scode = "  int K_T;")+ # number of features (minus 1 to exclude y)
                           stanvar(as.matrix(X%>%select(-y)), name = "X_T",scode = "  matrix[N_T,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(X$y, name = "y_T",scode = "  vector[N_T] y_T;")+# here we add the target samples y directly to the model
                           stanvar(as.integer(L_model), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(T_model, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(alpha_model, name = "alpha_T",scode = "  real alpha_T;")+ # alpha parameter fo the dirichlet prior for P
                           stanvar(grainsize_model, name = "grainsize_T",scode = "  int grainsize_T;") #suggested grainsize for the reduce_sum for the tree
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           #,prior("C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           ,prior("target += reduce_sum(partial_sum, to_array_2d( append_col(y_T,X_T)), grainsize_T,H,C, mu_T, sigma_T,K_T,L_T,NW,NL,T_T); // the main LogLikelihood calculation, parallelised via reduce_sum", check = FALSE)
           #,prior("C[1] ~ normal(-1,0.001);", check = FALSE)
           #,prior("C[2] ~ normal(1,0.001);", check = FALSE)
           #,prior("C[3] ~ normal(0,0.001);", check = FALSE)
           #,prior("mu_T[1] ~ normal(-1,0.001);", check = FALSE)
           #,prior("mu_T[2] ~ normal(-0.5,0.001);", check = FALSE)
           #,prior("mu_T[3] ~ normal(0.25,0.001);", check = FALSE)
           #,prior("mu_T[4] ~ normal(0.75,0.001);", check = FALSE)
           ,prior("sigma_T ~ lognormal(0,1);", check = FALSE)
           )
# 
```


Lets try to compile it.  


```{r eval=FALSE, include=FALSE}
BRMS_L2 =brm(formula
                               , stanvars = stanvars_gumbeltree
                               ,sample_prior="only"# ignore the brms model entirely
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,cores = 20
                               ,iter = 500
                               ,warmup = 150
                               ,chains = 4
                               ,refresh =50
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors
) 

save(BRMS_L2, file = "BRMS_L2.RData")
```
now for some plots


```{r}
library(tidybayes)

load("BRMS_L2.RData")
model = BRMS_L2
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
N = model$stanvars$N_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))

Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}

```

```{r}
#some plots


quantiles = array(dim = c(N,3))

for (i in 1:N){
  quantiles[i,] = quantile(mu_X[i,], probs = c(0.01,0.5,0.99))
}
plot(x=X$x1, y=quantiles[,1],  ylim=c(-2,2), col="green")
lines(x=X$x1, y=quantiles[,2], type = "p", col="orange")
lines(x=X$x1, y=quantiles[,3], type = "p", col="blue")
lines(x=X$x1, y=X$y, col="red",type = "p")

```
Great. A two level tree is working now. Lets test a 2 dimensional f(x) and a 4 level tree.

```{r}
N=1000
## a new sample of X
  X = data.frame(x1=rep(100,N)#column of same high number, this is the feature that does nothing
               ,x2=(runif(N)-0.5)*4#normalised features 2-10
               ,x3=(runif(N)-0.5)*4
               ,x4=(runif(N)-0.5)*4
               ,x5=(runif(N)-0.5)*4
               #,x6=rnorm(1000,0,1)
               #,x7=rnorm(1000,0,1)
               #,x8=rnorm(1000,0,1)
               #,x9=rnorm(1000,0,1)
               #,x10=rnorm(1000,0,1)
               )%>%mutate( y = rnorm(N,0,0.01)+sin(x2)+cos(x3))
library(plotly)
X%>%plot_ly(x=~x2,y=~x3,z=~y, type = "scatter3d",size=2) #show the function as a plot

```

```{r}

# define tree parameters
T_model = 0.1 # the temperature of the gumbel softmax. 0 ~ one-hot, inf. ~ uniform. T=1 is a good starting point
alpha_model = 0.2 # temperature of the dirichlet. 
L_model = 4 # number of levels in the tree
grainsize_model = N/10 # how many rows per chunk to multithread the loglikelihood calculation


stanvars_gumbeltree$N_T$sdata=length(X$y)
stanvars_gumbeltree$K_T$sdata= as.integer(length(X[1,])-1)
stanvars_gumbeltree$X_T$sdata=as.matrix(X%>%select(-y))
stanvars_gumbeltree$y_T$sdata=X$y
stanvars_gumbeltree$L_T$sdata=as.integer(L_model)
stanvars_gumbeltree$T_T$sdata=T_model
stanvars_gumbeltree$alpha_T$sdata=alpha_model
stanvars_gumbeltree$grainsize_T$sdata=grainsize_model
```


Lets try to compile it.  


```{r eval=FALSE, include=FALSE}
BRMS_2df =brm(y~1
                               , stanvars = stanvars_gumbeltree
                               ,sample_prior="only"# ignore the brms model entirely
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,cores = 20
                               ,iter = 500
                               ,warmup = 150
                               ,chains = 4
                               ,refresh =50
                               ,max_treedepth=20# reduced treedepth
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors
) 

save(BRMS_2df, file = "BRMS_2df.RData")
```
now for some plots


```{r}
library(tidybayes)

load("BRMS_2df.RData")
model = BRMS_2df
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
N = model$stanvars$N_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))

Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}

```

```{r}
#some plots


quantiles = array(dim = c(N,3))

for (i in 1:N){
  quantiles[i,] = quantile(mu_X[i,], probs = c(0.01,0.5,0.99))
}
X_plot = X%>%mutate(p01 = quantiles[,1],p50 = quantiles[,2],p99 = quantiles[,3])
X_plot%>%plot_ly(x=~x2, y=~x3, z=~y, type = "mesh3d", name = "Actual" )%>%
add_trace(x=~x2, y=~x3, z=~p01, type = "mesh3d", opacity=0.5, name = "fitted at 1%")%>%
add_trace(x=~x2, y=~x3, z=~p99, type = "mesh3d", opacity=0.5, name = "fitted at 99%") %>% layout(showlegend = TRUE
                                                                                                 ,legend = list(x = 0.1, y = 0.9, z = 0.9)
                                                                                                 ,title = "Actual vs Fitted")
  

```
Orange and green encase blue so the fit is good.
I think we can conclude the basic tree model and move onto BART and boosting.

## Advance BRMS to handle X and Y

On reflection, for full utility of BRMS for prediction and LOOIC for model fits, it is necessary for BRMS to know both y and X. We can add these modifications if we pass

```{r}

library(brms)
gumbeltree_xy <- custom_family(
  "gumbeltree_xy", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called mu,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),# sigma needs a log link function to stay positive

  type = "real",
  vars = c("mu_X[n]") # estimates mean for a given X[i] is passed to the function
  #vars = c("grainsize_T")
)
library(brms)

# rewrite the code such that the mu_T is calculated in the transformed parameters section getting rid of H

stan_funs_gumbeltree_xy <- "

vector mu_X_calc(matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , int N
                  ) {
  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in 1:L){
    c2=1;
    c=1+L-j; // need c to be the lowest level of the tree working upwards
    for (l in 1:c){
        c2 = c2*2;// can only make integer c2 = 2^C inside a loop
    }
    
    for (k in 1:c2/2){
        temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
    }
  }

    return temp[1]';// this is mu_X, the estimate of Y given the tree 
}

real gumbeltree_xy_lpdf(real y, real sigma, real mu){
  return normal_lpdf(y|mu,sigma);// correctly identify mu parameter as sigma, mu_X as mu
}

"
stanvars_trf_data = " 
  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
  for (i in 1:N){
    X_T[i] = X_base[vint1[i]];// reordering X_base to match any reordering performed on y. BRMS pre-processing of y generally reorders the input data.
  }
"

 stanvars_likelihood = "
  mu_X[start:end] = mu_X_calc( X_T[start:end] ,H,C,mu_T,K_T,L_T,NW,NL,N );
  
 "
 
 stanvars_model = "
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency
  }
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree_xy <- stanvar(scode = stan_funs_gumbeltree_xy, block = "functions")+
                           stanvar(pll_args = "int NL", name = "NL",scode = "int NL;// # leaves",  block = "tdata")+
                           stanvar(pll_args = "int NW", name = "NW",scode = "int NW;// # splits",  block = "tdata")+
                           stanvar(pll_args = "matrix X_T", name = "X_T",scode = "matrix[N,K_T] X_T;// reorder X_base to match y using vint1",  block = "tdata")+# this passes the re-ordered design matrix so that rows match with y (as y has likely been reordered)
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = "vector[NW] C; // cutoff level for the feature chosen at each node", name = "C", pll_args = "vector C",block = "parameters")+
                           stanvar(scode = "  vector[NL] mu_T;// mean of Y at each leaf", name = "mu_T", pll_args = "vector mu_T", block = "parameters")+
                           stanvar(scode = "  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node", name = "G", block = "parameters")+
                           stanvar(scode = "simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex", name = "P", block = "parameters")+
                           stanvar(scode = "vector[end] mu_X;" , name = "mu_X",block = "likelihood")+
                           stanvar(scode = stanvars_likelihood, block = "likelihood")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node", pll_args = "matrix H", block = "model")+
                           stanvar(scode = stanvars_model, block = "model")+
                           stanvar(as.integer(length(X[1,])-1), name = "K_T",scode = "  int K_T;")+ # number of features in X_T
                           stanvar(as.matrix(X%>%select(-y)), name = "X_base",scode = "  matrix[N,K_T] X_base;")+# here we add the design matrix X directly to the model
                           stanvar(as.integer(4), name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(1, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(0.2, name = "alpha_T",scode = "  real alpha_T;") # alpha parameter fo the dirichlet prior for P
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors_xy = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           )
# 



```

write out the new code

```{r echo=TRUE}
 formula = bf(y|vint(row) ~  1)
 
 stancode_mu_X =make_stancode(formula
                               ,family = gumbeltree_xy # needed if you want to do something else with the data as well as the tree
                               ,stanvars = stanvars_gumbeltree_xy
                               ,data = X %>% select(y)%>%mutate(row=1:n())
                               #,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,iter = 20
                               ,warmup = 10
                               ,chains = 2
                               ,refresh =1
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors_xy
) 

stancode_mu_X # type out the stancode for bug checking before we try to run it

```
ok. now try to compile.

```{r eval=FALSE, include=FALSE}
options("cmdstanr_verbose"=FALSE)

BRMS_xy =brm(formula
                ,family = gumbeltree_xy # needed if you want to do something else with the data as well as the tree
                ,stanvars = stanvars_gumbeltree_xy
                ,data = X %>% select(y)%>%mutate(row=1:n())
                ,backend = "cmdstanr"
                ,threads = threading(20)
                ,cores =20
                ,iter = 50
                ,warmup = 20
                ,chains = 4
                ,refresh =10
                ,max_treedepth=10
                ,adapt_delta =0.90
                ,save_pars = save_pars(all=TRUE)# potentially add this so that we can get access to the other parameters added via stanvars
                #,inits = "0"
                #,thin =1
                ,prior = priors_xy
) 



save(BRMS_xy, file = "BRMS_xy.RData")
```

now see if we get the same results
```{r}
library(tidybayes)

load("BRMS_xy.RData")
model = BRMS_xy
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
N = max(model$data$row)
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))

Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}

```

```{r}
#some plots
library(plotly)

quantiles = array(dim = c(N,3))

for (i in 1:N){
  quantiles[i,] = quantile(mu_X[i,], probs = c(0.01,0.5,0.99))
}
X_plot = X%>%mutate(p01 = quantiles[,1],p50 = quantiles[,2],p99 = quantiles[,3])
X_plot%>%plot_ly(x=~x2, y=~x3, z=~y, type = "mesh3d", name = "Actual" )%>%
add_trace(x=~x2, y=~x3, z=~p01, type = "mesh3d", opacity=0.5, name = "fitted at 1%")%>%
add_trace(x=~x2, y=~x3, z=~p99, type = "mesh3d", opacity=0.5, name = "fitted at 99%") %>% layout(showlegend = TRUE
                                                                                                 ,legend = list(x = 0.1, y = 0.9, z = 0.9)
                                                                                                 ,title = "Actual vs Fitted")
  

```

Yey! Working first go. This is essentially a random forest approach as we are averaging together a bunch of trees. HMC is sampling trees that are a better fit more often, just like a bagging process might try to emulate.

NOw we can use a few of the functionalities of BRMS. Starting with fit statistics using LOOIC.

```{r}
loo(BRMS_xy)


```

So we need the loglikelihood function first. See https://cran.r-project.org/web/packages/brms/vignettes/brms_customfamilies.html for some details

```{r}


# extract 
sigma = base::array((model%>%spread_draws(Intercept)%>%rename(draw=.draw)%>%arrange(draw,NL))$Intercept, dim = c(1,draws))

log_lik_gumbeltree_xy <- function(i, prep) {
  sigma <- prep$dpars$mu
  X_base <- prep$data$X_base
  vint1 <- prep$data$vint1
  N <- prep$nobs
  P <- prep$dpars$P
  G <- prep$dpars$G
  C <- prep$dpars$C
  mu <- prep$dpars$mu_T
  K <- prep$data$K_T
  L <- prep$data$L_T
  
  print(str(prep))
  Temp <- prep$data$T_T
  
  y <- prep$data$Y[i]
  NL=2^L;
  NW=NL-1;
  X_T = matrix(0,nrow = N, ncol = K)
  
  for (i in 1:N){
    X_T[i] = X_base[vint1[i]]# reordering X_base to match any reordering performed on y. BRMS pre-processing of y generally reorders the input data.
  }
  
  mu_X = mu_X_calc( X_T
          ,  P
          ,  G
          ,  C
          ,  mu
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  dnorm(y,mu_X[i],sigma)
}

```

Looks like it can't be done because the additional parameters defined through stanvars are not passed to the loglikelihood function. We need to include X for prediction purposes

### Adding Leave One Out Crossvalidation to BRMS
Looking at the loo package. I can see that we need to add generated quantities. Best way to do this is share the calculation of the expected value of y given the tree and each set of features X[i]. 

I have decided to bite the bullet and try to do this with the map_rect function in stan which is a different function for parallel calculation. This way we calculate the expected set of y's once and use it for both the loglikelihood calculation of the model and to store the loglikelihood for each X[i].


```{r}

library(brms)
gumbeltree_xy <- custom_family(
  "gumbeltree_xy", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called mu,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),# sigma needs a log link function to stay positive

  type = "real",
  vars = c("log_lik[n]") # estimates mean for a given X[i] is passed to the function
  #vars = c("grainsize_T")
)
library(brms)

# rewrite the code such that the mu_T is calculated in the transformed parameters section getting rid of H

stan_funs_gumbeltree_xy <- "

vector lp_reduce( vector beta , vector theta , real[] xr , int[] xi ) {

  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  int K = xi[1];
  int L = xi[2];
  int NW = xi[3];
  int NL = xi[4];
  int Base_N = xi[5];
  int N = xi[6];
  matrix[N,K] X = to_matrix(xr[1:(N*K)],N,K);
  vector[N] Y = to_vector(xr[(N*K+1):(N*K+N)]);
  matrix[NW,K] H = to_matrix(beta[1:(NW*K)],NW,K);
  vector[NW] C = to_vector(beta[(NW*K+1):(NW*K+NW)]);
  vector[NL] mu = to_vector(beta[(NW*K+NW+1):(NW*K+NW+NL)]);
  real sigma = beta[NW*K+NW+NL+1];

  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in 1:L){
    c2=1;
    c=1+L-j; // need c to be the lowest level of the tree working upwards
    for (l in 1:c){
        c2 = c2*2;// can only make integer c2 = 2^C inside a loop
    }
    
    for (k in 1:c2/2){
        temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
    }
  }
  for (n in 1:N) {
    
    temp[1,n] = normal_lpdf(Y[n] | temp[1,n], sigma);
    
  }
  return temp[1]';// this is mu_X, the estimate of Y given the tree 
}



real gumbeltree_xy_lpdf(real y, real sigma, real log_lik){
  return log_lik;// loglikelihood already calculated in map_rect/lp_reduce function
}

"
stanvars_trf_data = " 
  int M = N/nshards;// number of samples of data in each shard
  int xi[nshards, 6];  // integer data to pass through
  real xr[nshards, M*K_T+M];// the block of X for each shard and the matching Y's
  vector[0] theta[nshards];// empty set of shard specific parameters

  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
  for (i in 1:N){
    X_T[i] = X_base[vint1[i]];// reordering X_base to match any reordering performed on y. BRMS pre-processing of y generally reorders the input data.
  }


  // split into shards
  for ( i in 1:nshards ) {
    int j = 1 + (i-1)*M;
    int k = i*M;
    xr[i] = to_array_1d(append_row(to_vector(X_T[j:k]),Y[j:k]));
    xi[i,1] = K_T;
    xi[i,2] = L_T;
    xi[i,3] = NW;
    xi[i,4] = NL;
    xi[i,5] = N;
    xi[i,6] = M;
  }
"

 
 stanvars_tpar = "
 
   for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency
   }
 
 log_lik = map_rect( lp_reduce , append_row(append_row(append_row(to_vector(H),C),mu_T),exp(Intercept)) , theta , xr , xi ); // the expected value of y[i] for X[i]
 
 "
 

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree_xy <- stanvar(scode = stan_funs_gumbeltree_xy, block = "functions")+
                           stanvar(#pll_args = "int NL"
                                    name = "NL"
                                   ,scode = "int NL;// # leaves"
                                   ,  block = "tdata")+
                           stanvar(#pll_args = "int NW"
                                    name = "NW"
                                   ,scode = "int NW;// # splits"
                                   ,  block = "tdata")+
                           stanvar(#pll_args = "matrix X_T"
                                    name = "X_T"
                                   ,scode = "matrix[N,K_T] X_T;// reorder X_base to match y using vint1"
                                   ,  block = "tdata")+# this passes the re-ordered design matrix so that rows match with y (as y has likely been reordered)
                           stanvar(scode = stanvars_trf_data
                                   , block = "tdata")+
                           stanvar(scode = "vector[NW] C; // cutoff level for the feature chosen at each node"
                                   , name = "C"
                                   #, pll_args = "vector C"
                                   ,block = "parameters")+
                           stanvar(scode = "  vector[NL] mu_T;// mean of Y at each leaf"
                                   , name = "mu_T"
                                   #, pll_args = "vector mu_T"
                                   , block = "parameters")+
                           stanvar(scode = "  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node"
                                   , name = "G"
                                   , block = "parameters")+
                           stanvar(scode = "simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex"
                                   , name = "P"
                                   , block = "parameters")+
                           stanvar(scode = "vector[N] log_lik; // pointwise log-likelihood for LOO" 
                                   , name = "log_lik"
                                   , pll_args = "vector log_lik"
                                   ,block = "tparameters")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node"
                                   #, pll_args = "matrix H"
                                   , name = "H"
                                   , block = "tparameters")+
                           stanvar(scode = stanvars_tpar
                                   , block = "tparameters")+
                           stanvar(as.integer(length(X[1,])-1)
                                   , name = "K_T"
                                   ,scode = "  int K_T;")+ # number of features in X_T
                           stanvar(as.matrix(X%>%select(-y))
                                   , name = "X_base"
                                   ,scode = "  matrix[N,K_T] X_base;")+# here we add the design matrix X directly to the model
                           stanvar(as.integer(4)
                                   , name = "L_T"
                                   ,scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(1
                                   , name = "T_T"
                                   ,scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(0.2
                                   , name = "alpha_T"
                                   ,scode = "  real alpha_T;")+ # alpha parameter fo the dirichlet prior for P
                           stanvar(20
                                   , name = "nshards"
                                   ,scode = "  int nshards;")# map_rect function # slices of the data to parallelise
   
   
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors_xy = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("for (i in 1:NW){ to_vector(P[i]) ~ dirichlet(rep_vector(alpha_T,K_T));}//P also needs a prior and should have a U shaped dirichlet", check = FALSE)
           )
# 



```

write out the new code

```{r echo=TRUE}


 stancode_xy =make_stancode(y|vint(row)  ~  1
                               ,family = gumbeltree_xy # needed if you want to do something else with the data as well as the tree
                               ,stanvars = stanvars_gumbeltree_xy
                               ,data = X %>% mutate(row=1:n())
                               #,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,iter = 20
                               ,warmup = 10
                               ,chains = 2
                               ,refresh =1
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors_xy
) 

stancode_xy # type out the stancode for bug checking before we try to run it

```
ok. now try to compile.

```{r eval=FALSE, include=FALSE}
options("cmdstanr_verbose"=FALSE)

BRMS_xy =brm(y |vint(row)~1
                ,family = gumbeltree_xy # needed if you want to do something else with the data as well as the tree
                ,stanvars = stanvars_gumbeltree_xy
                ,data = X %>% select(y)%>%mutate(row=1:n())
                ,backend = "cmdstanr"
                ,threads = threading(20)
                ,cores =20
                ,iter = 200
                ,warmup = 150
                ,chains = 4
                ,refresh =10
                ,max_treedepth=20
                ,adapt_delta =0.99
                ,save_pars = save_pars(all=TRUE)# potentially add this so that we can get access to the other parameters added via stanvars
                #,inits = "0"
                #,thin =1
                ,prior = priors_xy
) 



save(BRMS_xy, file = "BRMS_xy.RData")
```

now see if we get the same results
```{r}
library(tidybayes)

load("BRMS_xy.RData")
model = BRMS_xy
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
N = max(model$data$row)
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))

Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}

```

```{r}
#some plots
library(plotly)

quantiles = array(dim = c(N,3))

for (i in 1:N){
  quantiles[i,] = quantile(mu_X[i,], probs = c(0.01,0.5,0.99))
}
X_plot = X%>%mutate(p01 = quantiles[,1],p50 = quantiles[,2],p99 = quantiles[,3])
X_plot%>%plot_ly(x=~x2, y=~x3, z=~y, type = "mesh3d", name = "Actual" )%>%
add_trace(x=~x2, y=~x3, z=~p01, type = "mesh3d", opacity=0.5, name = "fitted at 1%")%>%
add_trace(x=~x2, y=~x3, z=~p99, type = "mesh3d", opacity=0.5, name = "fitted at 99%") %>% layout(showlegend = TRUE
                                                                                                 ,legend = list(x = 0.1, y = 0.9, z = 0.9)
                                                                                                 ,title = "Actual vs Fitted")
  

```

Now we try to perform a loocv calculation

```{r}

library(loo)

log_lik = base::array((model%>%spread_draws(log_lik[i])%>%arrange(.iteration,.chain,i))$log_lik, dim = c(50,4,N))

rel_n_eff <- relative_eff(exp(log_lik))
loo(log_lik, r_eff = rel_n_eff, cores = 20)

```
OK, it works but it is slow. Lets see if I can speed it up by removing the need for Y to be passed to brms i.e. I bypass the BRMS implementation of the loglikelihood.

```{r}

library(brms)


# rewrite the code such that the mu_T is calculated in the transformed parameters section getting rid of H

stan_funs_gumbeltree_loo <- "

real partial_sum(real[,] xy_slice,
                   int start,
                   int end,
                   matrix H,
                   vector C,
                   vector mu,
                   real sigma,
                   int K,
                   int L,
                   int NW,
                   int NL,
                   real T) {

  real y[end-start+1] = to_array_1d(xy_slice[,1]);
  matrix[end-start+1 , K] X = to_matrix(xy_slice[,2:(K+1)]);
  matrix[NW,end-start+1] w; //calculation of the weight for each node for each sample
  matrix[NL,end-start+1] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,end-start+1)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,end-start+1) ;
  for (j in 1:L){
    c2=1;
    c=1+L-j; // need c to be the lowest level of the tree working upwards
    for (l in 1:c){
        c2 = c2*2;// can only make integer c2 = 2^C inside a loop
    }
    
    for (k in 1:c2/2){
        temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
    }
  }

    return normal_lpdf(y | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }

vector lp_reduce( vector beta , vector theta , real[] xr , int[] xi ) {

  // we use this version of the function when calculating mu_T in transformed parameters so
  // that we don't have to declare all the additional variables in trf param section as well
  int K = xi[1];
  int L = xi[2];
  int NW = xi[3];
  int NL = xi[4];
  int Base_N = xi[5];
  int N = xi[6];
  matrix[N,K] X = to_matrix(xr[1:(N*K)],N,K);
  vector[N] Y = to_vector(xr[(N*K+1):(N*K+N)]);
  matrix[NW,K] H = to_matrix(beta[1:(NW*K)],NW,K);
  vector[NW] C = to_vector(beta[(NW*K+1):(NW*K+NW)]);
  vector[NL] mu = to_vector(beta[(NW*K+NW+1):(NW*K+NW+NL)]);
  real sigma = beta[NW*K+NW+NL+1];

  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in 1:L){
    c2=1;
    c=1+L-j; // need c to be the lowest level of the tree working upwards
    for (l in 1:c){
        c2 = c2*2;// can only make integer c2 = 2^C inside a loop
    }
    
    for (k in 1:c2/2){
        temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
    }
  }
  for (n in 1:N) {
    
    temp[1,n] = normal_lpdf(Y[n] | temp[1,n], sigma);
    
  }
  return temp[1]';// this is mu_X, the estimate of Y given the tree 
}



real gumbeltree_xy_lpdf(real y, real sigma, real log_lik){
  return log_lik;// loglikelihood already calculated in map_rect/lp_reduce function
}

"
stanvars_trf_data = " 
  int M = N_T/nshards;// number of samples of data in each shard
  int xi[nshards, 6];  // integer data to pass through
  real xr[nshards, M*K_T+M];// the block of X for each shard and the matching Y's
  vector[0] theta[nshards];// empty set of shard specific parameters

  NL=1;
  for (i in 1:L_T){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  NW=NL-1;
  
  // split into shards
  for ( i in 1:nshards ) {
    int j = 1 + (i-1)*M;
    int k = i*M;
    xr[i] = to_array_1d(append_row(to_vector(X_T[j:k]),Y_T[j:k]));
    xi[i,1] = K_T;
    xi[i,2] = L_T;
    xi[i,3] = NW;
    xi[i,4] = NL;
    xi[i,5] = N;
    xi[i,6] = M;
  }
"

 
 stanvars_tpar = "
 
   for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T_T^2 + T_T +1) / (T_T+1) + G[j])/T_T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1). In model section for computational efficiency
   }
 
 
 "
 stanvars_gen = "
 vector[N] log_lik;
 
 log_lik = map_rect( lp_reduce , append_row(append_row(append_row(to_vector(H),C),mu_T),sigma_T) , theta , xr , xi ); 
 
 
 "

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irrelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree_loo <- stanvar(scode = stan_funs_gumbeltree_loo, block = "functions")+
                           stanvar(#pll_args = "int NL"
                                    name = "NL"
                                   ,scode = "int NL;// # leaves"
                                   ,  block = "tdata")+
                           stanvar(#pll_args = "int NW"
                                    name = "NW"
                                   ,scode = "int NW;// # splits"
                                   ,  block = "tdata")+
                           stanvar(scode = stanvars_trf_data
                                   , block = "tdata")+
                           stanvar(scode = "vector[NW] C; // cutoff level for the feature chosen at each node"
                                   , name = "C"
                                   #, pll_args = "vector C"
                                   ,block = "parameters")+
                           stanvar(scode = "  vector[NL] mu_T;// mean of Y at each leaf"
                                   , name = "mu_T"
                                   #, pll_args = "vector mu_T"
                                   , block = "parameters")+
                           stanvar(scode = "  real<lower=0> sigma_T;// expected error between y[i] and mu_T"
                                   , name = "sigma_T"
                                   , block = "parameters")+
                           stanvar(scode = "  vector[K_T] G[NW];//number of gumbel samples needed to select a feature at each node"
                                   , name = "G"
                                   , block = "parameters")+
                           stanvar(scode = "simplex[K_T] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex"
                                   , name = "P"
                                   , block = "parameters")+
                           stanvar(scode = "matrix[NW,K_T] H; // one hot to select a feature at each node"
                                   #, pll_args = "matrix H"
                                   , name = "H"
                                   , block = "tparameters")+
                           stanvar(scode = stanvars_tpar
                                   , block = "tparameters")+
                           stanvar(scode = stanvars_gen
                                   , block = "genquant")+
                           stanvar(as.integer(length(X[1,])-1)
                                   , name = "K_T"
                                   ,scode = "  int K_T;")+ # number of features in X_T
                           stanvar(1000
                                   , name = "N_T"
                                   ,scode = "  int N_T;")+
                           stanvar(as.matrix(X%>%select(-y))
                                   , name = "X_T"
                                   ,scode = "  matrix[N_T,K_T] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(X$y
                                   , name = "Y_T"
                                   ,scode = " vector[N_T] Y_T;")+
                           stanvar(as.integer(4)
                                   , name = "L_T"
                                   ,scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(1
                                   , name = "T_T"
                                   ,scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(0.2
                                   , name = "alpha_T"
                                   ,scode = "  real alpha_T;")+ # alpha parameter fo the dirichlet prior for P
                           stanvar(20
                                   , name = "nshards"
                                   ,scode = "  int nshards;")# map_rect function # slices of the data to parallelise
                           
  
 # we use the priors function with check = false to push our additional priors and main LL function into the stancode.
 priors_loo = c(prior(" for (i in 1:NW){G[i] ~ gumbel(0,1);} // each vector of gumbels used to create a one-hot", check = FALSE)
           ,prior("C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest", check = FALSE)
           ,prior("target += reduce_sum(partial_sum, to_array_2d( append_col(Y_T,X_T)), nshards,H,C, mu_T, sigma_T,K_T,L_T,NW,NL,T_T); // the main LogLikelihood calculation, parallelised via reduce_sum", check = FALSE)# here we add the main log likelihood to bypass brms
           )



```

write out the new code

```{r echo=TRUE}


 stancode_loo =make_stancode(y  ~  1
                               ,stanvars = stanvars_gumbeltree_loo
                               ,data = data.frame(y=1)
                               #,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,iter = 20
                               ,warmup = 10
                               ,chains = 2
                               ,refresh =1
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = priors_loo
) 

stancode_loo # type out the stancode for bug checking before we try to run it

```
ok. now try to compile.

```{r eval=FALSE, include=FALSE}
options("cmdstanr_verbose"=FALSE)

BRMS_loo =brm(y ~1
                ,stanvars = stanvars_gumbeltree_loo
                ,data = data.frame(y=1)
                ,sample_prior="only"# ignore the brms model entirely
                ,backend = "cmdstanr"
                ,threads = threading(20)
                ,cores =20
                ,iter = 200
                ,warmup = 150
                ,chains = 4
                ,refresh =10
                ,max_treedepth=20
                ,adapt_delta =0.95
                #,inits = "0"
                #,thin =1
                ,prior = priors_loo
) 



save(BRMS_loo, file = "BRMS_loo.RData")
```

now see if we get the same results
```{r}
library(tidybayes)

load("BRMS_loo.RData")
model = BRMS_loo
L = model$stanvars$L_T$sdata
K = model$stanvars$K_T$sdata
N =  model$stanvars$N_T$sdata
NL=2^L
NW=NL-1
draws = nsamples(model)
P = base::array((model%>%spread_draws(P[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$P, dim = c(NW,K,draws))
G = base::array((model%>%spread_draws(G[NW,K])%>%rename(draw=.draw)%>%arrange(draw,K,NW))$G, dim = c(NW,K,draws))
C = base::array((model%>%spread_draws(C[NW])%>%rename(draw=.draw)%>%arrange(draw,NW))$C, dim = c(NW,draws))
mu = base::array((model%>%spread_draws(mu_T[NL])%>%rename(draw=.draw)%>%arrange(draw,NL))$mu_T, dim = c(NL,draws))

Temp = model$stanvars$T_T$sdata

# test it out
residual_test = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,1], nrow = NW)
          ,  matrix(G[,,1], nrow = NW)
          ,  as.vector(C[,1])
          ,  as.vector(mu[,1])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
str(residual_test)

```


```{r}
mu_X = array(dim = c(N,draws))
H = array(dim = c(NW,K,draws))
for(i in 1:draws){
  mu_X[,i] = mu_X_calc( as.matrix(X%>%select(-y))
          ,  matrix(P[,,i], nrow = NW)
          ,  matrix(G[,,i], nrow = NW)
          ,  as.vector(C[,i])
          ,  as.vector(mu[,i])
          ,  K
          ,  L
          ,  NW
          ,  NL
          ,  N
          ,  Temp
          )
  H[,,i]  = H_calc(matrix(G[,,i], nrow = NW)
               ,matrix(P[,,i], nrow = NW)
               ,NW,K,Temp)
}

```

```{r}
#some plots
library(plotly)

quantiles = array(dim = c(N,3))

for (i in 1:N){
  quantiles[i,] = quantile(mu_X[i,], probs = c(0.01,0.5,0.99))
}
X_plot = X%>%mutate(p01 = quantiles[,1],p50 = quantiles[,2],p99 = quantiles[,3])
X_plot%>%plot_ly(x=~x2, y=~x3, z=~y, type = "mesh3d", name = "Actual" )%>%
add_trace(x=~x2, y=~x3, z=~p01, type = "mesh3d", opacity=0.5, name = "fitted at 1%")%>%
add_trace(x=~x2, y=~x3, z=~p99, type = "mesh3d", opacity=0.5, name = "fitted at 99%") %>% layout(showlegend = TRUE
                                                                                                 ,legend = list(x = 0.1, y = 0.9, z = 0.9)
                                                                                                 ,title = "Actual vs Fitted")
  

```


Now we try to perform a loocv calculation

```{r}

library(loo)

log_lik = base::array((model%>%spread_draws(log_lik[i])%>%arrange(.iteration,.chain,i))$log_lik, dim = c(50,4,N))

rel_n_eff <- relative_eff(exp(log_lik))
loo(log_lik, r_eff = rel_n_eff, cores = 20)

```
Yey! Loo is working well.

We can now move on to boosting model, but we will define the basic stan model now as well, for those that don't need or want the hassle of working with BRMS

```{r}
tree = function(a,b){a*b}

formula = bf(y~inv_logit(a)+1/b, nl=TRUE)
formula = bf(y~to_vector(t,b),t~1+offset(a),nl=TRUE, loop=FALSE)
get_prior(formula
              ,data=data.frame(y=c(1,2,3,4),a=c(1,2,3,4),b=c(1,2,3,4)))
make_stancode(formula
              ,data=data.frame(y=c(1,2,3,4),a=c(1,2,3,4),b=c(1,2,3,4))
              , prior = c(prior(normal(0,1),nlpar = t)
                          #,prior(normal(0,1),nlpar = k)
                          )
              )


formula = bf(Incident ~ log(mi(rev)) + (1|Industry_Group) + (AY), family=poisson())+bf( rev | mi()~1 ,family=gaussian())
get_prior(formula
              ,data = data.frame(Incident=c(1,1,1,1),rev=c(1,NA,1,3),Industry_Group=c(1,2,1,3),AY=c(1,2,1,3) )
          )
model = brm(formula
               #,family = poisson()
               ,data = data.frame(Incident=c(1,1,1,1),rev=c(1,NA,1,3),Industry_Group=c(1,2,1,3),AY=c(1,2,1,3) )
            #, prior = c(prior(normal(0,1),nlpar = k)
            #              #,prior(normal(0,1),nlpar = k)
            #              )
)

make_stancode(formula
               #,family = poisson()
               ,data = data.frame(Incident=c(1,1,1,1),rev=c(1,NA,1,3),Industry_Group=c(1,2,1,3),AY=c(1,2,1,3) )
            , prior = c(prior("bsp_Incident[1]~normal(1,0.0000001)",check=FALSE)
            #              #,prior(normal(0,1),nlpar = k)
                          )
)


```

