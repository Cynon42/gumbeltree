---
title: "gumbeltree"
author: "cynon"
date: "3/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

My first R Markdown file, but trying to illustrate a relatively advanced topic that does not exist in the literature.
We will attempt to use rebar-concrete, or in other words the Gumbel Softmax function to approximate discrete choices within an HMC using Stan.

I have created an example decision tree as I see it working and we will try to improve thie tree to include implementations of either boosted trees or bayesian additive regression trees as we progress

## basic code

I put the actual stancode below, but first I set up the cmdstanr engine for processinf cmdstanr stancode as it is more up to date at the moment.

```{r, eval = FALSE}

# this code chunk is made redundant by the installation of the github versions on BRMS and cmdstanr into the docker image.
# see the "Dockerfile"
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
library(cmdstanr)
check_cmdstan_toolchain()
install_cmdstan(cores = 10,overwrite=TRUE)
cmdstan_path()
cmdstan_version()

file <- file.path(cmdstan_path(), "examples", "bernoulli", "bernoulli.stan")
mod <- cmdstan_model(file)

data_list <- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))

# confirming models can run and run in parallel
fit <- mod$sample(
  data = data_list,
  iter_warmup = 1000,
  iter_sampling = 1e5,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 500
)
```

Now get cmdstan code chunks working.

```{r}
library(cmdstanr)
library(knitr)

register_knitr_engine()
```


```{stan output.var="gumbeltree", eval = FALSE}

// This stan chunk results in a CmdStanModel object called "gumbeltree"

// here is an attemp to make a regression tree using
// gumbel softmax
data {
  int<lower=0> N;//number of rows or samples
  int K;// number of features including the extra feature of a column of ones
  vector[N] y;// the samples of the target function
  matrix[N,K] X; // the features or design matrix. continuous features normalised to normal(0,1)
  int L; // number of levels in the tree, order of tree
  real T; // temperature to run the gumbel softmax approximation of a one-hot
          // this is a critical hyperparameter that defines how easily the model can traverse between modes, 
          // or between trees that are a good fit, but are in different parts of parameter space
}
transformed data{
  // need to add column of ones to the design matrix
  // all binary features incl the column of ones need to be replaced with (-100,100) so that C will never be outside the range
  // gumbel parameters are k+1 as a result
  //gumbel softmax used to select a feature, 
  //or select "no feature" wich is to select the first feature which is always 1
  // this allows for an option that the node does not split i.e.
  // always select the left branch
  // if another feature is selected, then the logistic function is used
  // to find a cutoff point on that feature with which to branch the node
  //int constant=2;
  int NL=1;// number of possible leaves
  int NW;// number of nodes or weights or choices in the tree
  
  for (i in 1:L){
    NL = NL*2;// can only make integer 2^L inside a loop
  }
  
  NW=NL-1;
  
}

// The main problem is referring to the tree

parameters {
  real<lower=0> sigma;// assume gaussian error in y for a minute
  vector[K] G[NW];//number of gumbel samples needed to select a feature at each node
  simplex[K] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex
  
  real C[NW]; //for each feature selected we need a cutoff point
  real mu[NL];// mean of Y at each leaf
}
model {
  real F[N]; //the estimation of Y given X i.e. y^=F(X)
  real w[NW]; //calculation of the weight for each node for each sample
  real temp[NL]; //temporary store of tree calculations
  int c;// local integer parameter
  int c2;// local integer parameter
  for (i in 1:N){
    for (j in 1:NW){
      w[j]=inv_logit(10*(dot_product(softmax( (log(P[j])*(T^2 + T +1) / (T+1) + G[j])/T ),to_vector(X[i])) +C[j]));
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1)
      // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
    }
    // start at the bottom of the tree and work upwards
    // at the end of the loop temp[1] is the result of applying the tree to X[i]
    temp = mu ;
    for (j in L:1){
      c=1;
      for (k in 1:j-1){
        c = c*2;// can only make integer c = (2^(j-1)) inside a loop
      }    
      
      for (k in 1:c){
        c2=1;
        for (l in 1:j){
          c2 = c2*2;// can only make integer c2 = 2^j inside a loop
        }
        temp[k]=w[k+NW-c2+1]*temp[2*k-1]+(1-w[k+NW-c2+1])*temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
      }
    }
    F[i]=temp[1];
  }
  y ~ normal(F, sigma);// assuming that y is a function that ranges (-inf,+inf)
  for (i in 1:NW){
    G[i]~gumbel(0,1); // each vector of gumbels used to create a one-hot
  }
  C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest
  //P needs a prior and should have a U shaped simplex prior
}
```

now build the model
```{r}
file = file.path("gumbeltree.stan")
mod <- cmdstan_model(file)
```


## Synthetic data
I am trying to create normalised features for the most part. Categorical features converted to one-hot and normalised to something like (-100,100) so that the shift function C has no effect (as it is also normalised) and the logistic always returns one

```{r }
library(dplyr)
# generate some synthetic data
X = data.frame(x1=rep(100,1000)#column of same high number, this is the feature that does nothing
               ,x2=rnorm(1000,0,1)#normalised features 2-10
               ,x3=rnorm(1000,0,1)
               ,x4=rnorm(1000,0,1)
               ,x5=rnorm(1000,0,1)
               ,x6=rnorm(1000,0,1)
               ,x7=rnorm(1000,0,1)
               ,x8=rnorm(1000,0,1)
               ,x9=rnorm(1000,0,1)
               ,x10=rnorm(1000,0,1))%>%
  mutate(y = 3*x2+x3+sin(x4)+cos(x5)+x6*x7+0.01*x8)# a function to try to approximate
```

now try the code with the data

```{r}
standata = list(
  N = length(X$y)
  ,K = length(X[1,])-1
  ,y = X$y
  ,X = X%>%select(-y)
  ,L=4
  ,T=1
)

fit <- mod$sample(
  data = standata,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 100,
  iter_sampling = 1000,
  refresh = 5
)

```

this didn't work very well. We update the code a bit:

```{stan output.var="gumbeltree"}

// This stan chunk results in a CmdStanModel object called "gumbeltree"

// here is an attemp to make a regression tree using
// gumbel softmax
data {
  int<lower=0> N;//number of rows or samples
  int K;// number of features
  vector[N] y;// the samples of the target function
  matrix[N,K] X; // the features or design matrix. continuous features normalised to normal(0,1)
  int L; // number of levels in the tree, order of tree
  real T; // temperature to run the gumbel softmax approximation of a one-hot
          // this is a critical hyperparameter that defines how easily the model can traverse between modes,
          // or between trees that are a good fit, but are in different parts of parameter space
}
transformed data{
  // need to add column of ones to the design matrix
  // all binary features incl the column of ones need to be replaced with (-100,100) so that C will never be outside the range
  // gumbel parameters are k+1 as a result
  //gumbel softmax used to select a feature,
  //or select "no feature" wich is to select the first feature which is always 1
  // this allows for an option that the node does not split i.e.
  // always select the left branch
  // if another feature is selected, then the logistic function is used
  // to find a cutoff point on that feature with which to branch the node
  //int constant=2;
  int NL=1;// number of possible leaves
  int NW;// number of nodes or weights or choices in the tree

  for (i in 1:L){
    NL = NL*2;// can only make integer 2^L inside a loop
  }

  NW=NL-1;

}

// The main problem is referring to the tree

parameters {
  real<lower=0> sigma;// assume gaussian error in y for a minute
  vector[K] G[NW];//number of gumbel samples needed to select a feature at each node
  simplex[K] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex

  vector[NW] C; //for each feature selected we need a cutoff point
  vector[NL] mu;// mean of Y at each leaf
}
model {
  vector[N] F; //the estimation of Y given X i.e. y^=F(X)
  matrix[NW,K] H; // one hot to select a feature at each node
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter
  for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T^2 + T +1) / (T+1) + G[j])/T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1)
  }
  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

  y ~ normal(temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  for (i in 1:NW){
    G[i]~gumbel(0,1); // each vector of gumbels used to create a one-hot
  }
  C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest
  //P needs a prior and should have a U shaped simplex prior
}

```

now build the updated model
```{r}
file = file.path("gumbeltree2.stan")
gumbeltree <- cmdstan_model(file)
```
fit the new model
```{r}

fit <- gumbeltree$sample(
  data = standata,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 200,
  iter_sampling = 1000,
  max_treedepth = 20,
  refresh = 50
)

```

a successful fit! :)

```{r}
library(shinystan)
library(rstan)
stanfit <- rstan::read_stan_csv(fit$output_files())
launch_shinystan(stanfit)
```
so it worked, but I'm not sure it worked very well. lets use a larger tree.

```{r}
standata = list(
  N = length(X$y)
  ,K = length(X[1,])-2
  ,y = X$y
  ,X = X%>%select(-1,-y)
  ,L=5 # larger tree
  ,T=1
)

fit <- gumbeltree$sample(
  data = standata,
  seed = 123,
  chains = 4,
  parallel_chains = 10,
  iter_warmup = 200,
  iter_sampling = 200,
  max_treedepth = 20,
  adapt_delta = 0.95,
  refresh = 10
)

```
did this one work better? It runs slow. We can try speeding it up with GPU. That didn't work. So we try reduce_sum to increase parallelisation.

The new code is:
```{stan output.var="gumbeltree_reduce"}

// This stan chunk results in a CmdStanModel object called "gumbeltree"

// here is an attemp to make a regression tree using
// gumbel softmax

functions {
  real partial_sum(vector y_slice,
                  int start,
                   int end,
                   matrix X,
                   matrix H,
                   vector C,
                   vector mu,
                   real sigma,
                   int K,
                   int L,
                   int NW,
                   int NL,
                   real T) {

  matrix[NW,end-start+1] w; //calculation of the weight for each node for each sample
  matrix[NL,end-start+1] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X[start:end]')+rep_matrix(C,end-start+1)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,end-start+1) ;
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return normal_lpdf(y_slice | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }
}

data {
  int<lower=0> N;//number of rows or samples
  int K;// number of features
  vector[N] y;// the samples of the target function
  matrix[N,K] X; // the features or design matrix. continuous features normalised to normal(0,1)
  int L; // number of levels in the tree, order of tree
  real T; // temperature to run the gumbel softmax approximation of a one-hot
          // this is a critical hyperparameter that defines how easily the model can traverse between modes,
          // or between trees that are a good fit, but are in different parts of parameter space
  int grainsize; //number of rows per parallel process in reduce_sum
}
transformed data{
  // need to add column of ones to the design matrix
  // all binary features incl the column of ones need to be replaced with (-100,100) so that C will never be outside the range
  // gumbel parameters are k+1 as a result
  //gumbel softmax used to select a feature,
  //or select "no feature" wich is to select the first feature which is always 1
  // this allows for an option that the node does not split i.e.
  // always select the left branch
  // if another feature is selected, then the logistic function is used
  // to find a cutoff point on that feature with which to branch the node
  //int constant=2;
  int NL=1;// number of possible leaves
  int NW;// number of nodes or weights or choices in the tree

  for (i in 1:L){
    NL = NL*2;// can only make integer 2^L inside a loop
  }

  NW=NL-1;

}

// The main problem is referring to the tree

parameters {
  real<lower=0> sigma;// assume gaussian error in y for a minute
  vector[K] G[NW];//number of gumbel samples needed to select a feature at each node
  simplex[K] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex

  vector[NW] C; //for each feature selected we need a cutoff point
  vector[NL] mu;// mean of Y at each leaf
}
model {

  matrix[NW,K] H; // one hot to select a feature at each node
  matrix[NW,N] w; //calculation of the weight for each node for each sample
  matrix[NL,N] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter
  for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T^2 + T +1) / (T+1) + G[j])/T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1)
  }

  target += reduce_sum(partial_sum, y,
                       grainsize,
                       X,
                       H,
                       C,
                       mu,
                       sigma,
                       K,
                       L,
                       NW,
                       NL,
                       T);

  for (i in 1:NW){
    G[i]~gumbel(0,1); // each vector of gumbels used to create a one-hot
  }
  C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest
  //P needs a prior and should have a U shaped simplex prior
}

```

Build the new model

```{r}
file = file.path("gumbeltree_reduce.stan")
gumbeltree <- cmdstan_model(file,cpp_options = list(stan_threads = TRUE))
```


running the new version with reduce_sum:

```{r}
standata = list(
  N = length(X$y)
  ,K = length(X[1,])-2
  ,y = X$y
  ,X = X%>%select(-1,-y)
  ,L=5 # larger tree
  ,T=1
  ,grainsize=100
)

fit <- gumbeltree$sample(
  data = standata,
  seed = 123,
  chains = 4,
  parallel_chains = 20,
  threads_per_chain =20,
  iter_warmup = 200,
  iter_sampling = 200,
  max_treedepth = 15,
  adapt_delta = 0.95,
  refresh = 10
)

```


Lets have a look on shinystan
```{r}
library(shinystan)
library(rstan)
stanfit <- rstan::read_stan_csv(fit$output_files())
launch_shinystan(stanfit)
```

run a bigger tree to see if there are any issues and if the residuals might get better

```{r}
standata = list(
  N = length(X$y)
  ,K = length(X[1,])-1
  ,y = X$y
  ,X = X%>%select(-y)
  ,L=6 # larger tree
  ,T=1
  ,grainsize=100
)

fit2 <- gumbeltree$sample(
  data = standata,
  seed = 123,
  chains = 2,
  parallel_chains = 20,
  threads_per_chain =20,
  iter_warmup = 200,
  iter_sampling = 200,
  max_treedepth = 17,
  adapt_delta = 0.99,
  refresh = 10
)
stanfit <- rstan::read_stan_csv(fit2$output_files())
save(fit2,stanfit, file = "fit2_stanfit.RData")

```

lets look at shinystan on the larger tree model

```{r}
library(shinystan)
library(rstan)
launch_shinystan(stanfit)
```

## Some testing of the math

assuming models have gone well, we should just check out the math a bit to see that it works as intended.

first lets sample some Gumbel distributions. an interesting reference might be:
https://github.com/howardnewyork/rebar/blob/master/rebar_1.stan


```{r}
# gumbel samples for 10 dimensional one-hot
G = -log(-log(runif(10)))


```

this should be able to be transformed into an approximate one-hot sample using the formula in the stan model. lets try to do this in R.

```{r}
# stan code is: H[j] = softmax( (log(P[j])*(T^2 + T +1) / (T+1) + G[j])/T)'

# our version is
T = 1 # an idea of how accute to exponentiate the gumbels. translates into how close to a real one-hot we will be
one_hot = data.frame(G=-log(-log(runif(10)))
                     ,P=rep(0.1,10))%>% # P is the probability simplex of scoring a 1 in any given position
          mutate(OH = exp((log(P)*(T^2+T+1)/(T+1)+G)/T) )%>%
          mutate(OH = OH/sum(OH))
hist(one_hot$OH)

```
Now expand this for 1000 samples

```{r}
T=1 # T=0 is one-hot, T=infinity gives samples like P
one_hot = data.frame(G=-log(-log(runif(10*1000)))
                     ,P=rep(c( 0.1/9
                              ,0.1/9
                              ,0.1/9
                              ,0.1/9
                              ,0.1/9
                              ,0.1/9
                              ,0.1/9
                              ,0.1/9
                              ,0.1/9
                              ,0.9)
                              ,1000),dimension = rep(1:10,1000))%>% # P is the probability simplex of scoring a 1 in any given position
          mutate(OH = exp((log(P)*(T^2+T+1)/(T+1)+G)/T),sample = 1:n() %/% 10 +1 )%>%
          group_by(sample)%>%
          mutate(OH = OH/sum(OH))
density = one_hot %>%group_by(dimension)%>%summarise(y = density(OH,n=100)$y,x=density(OH,n=100)$x)

library(plotly)
density%>%arrange(dimension,x)%>%plot_ly(x=~x,y=~y, type="scatter",name = "Gumbel-Softmax",color = ~dimension )%>%
  layout(yaxis = list(range = c(0, 10)))

```
maybe a histogram would be better?
```{r}
one_hot%>%plot_ly(x=~OH, type="histogram",color=~dimension,bingroup=1)%>%
  layout(
  barmode="stack",
  bargap=0.1,yaxis = list(range = c(0, 1000)))

```

nope. I will transform OH to an integer first then

```{r}
 one_hot %>% mutate(OH = (OH*10)%/%1) %>%filter(dimension %in% c(1,10))%>% plot_ly(x=~OH, type="histogram",color=~dimension)%>%
  layout(
  barmode="stack",
  bargap=0.1,yaxis = list(range = c(0, 1000)))

```

looks good. So back to residual plotting.

## Calculating Residuals
first need to extract the model parameters and inspect the resulting tree and how it fits the data. Perhaps the function is too complex to be fitted by the size of tree, but it will be interesting to see what happens if we do this with a simplified function of X.

```{r}
library(tidybayes)
fitted_model = X%>%add_fitted_draws(stanfit,dpar=TRUE)


```
doesn't work. I can try a different draws function, which would take a few minutes, or I could try to put this into a BRMS family, which would be a cool way of implementing these tree functions. I figured out how to do it, but when I thought about it, the stancode would be materially slower. GPU accelleration is coming to cmdstan in the near future. I'm not so sure about implementations within BRMS being able to use GPU so quickly. Rstan will also need to wait while cmdstanr is released. So it might be several months before BRMS code can make use of GPU accelleration, which is where this model should really be implemented.

## have a got at formulation in BRMS

I will provide the BRMS implementation prototype here for completeness, but I think it will be more useful in future when the tree families can be used in conjunction with other families in the same model e.g. a tree for data infilling, and then a non-linear GAM for frequency/severity models. This capability would be invaluable as the BRMS ability to create stancode for several prototype models relatively quickly is the best method in my opinion.

first we would normally need to install development versions of the cmdstanr and BRMS packages. This is already incorporated into the dockerfile.

```{r, eval = FALSE}
# this contains redundant lines that may be needed in various cases. Approach varies with OS and environment.

#library(devtools)
#remove.packages(c("StanHeaders", "rstan"))
##install_github("hsbadr/rstan/StanHeaders@develop")
#install.packages("StanHeaders", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
#install.packages("RcppEigen", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
#install_version("RcppEigen", "0.3.3.9.1")
##install_github("hsbadr/rstan/rstan/rstan@develop")
#install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
#install_github("paul-buerkner/brms",dependencies = TRUE)

```

```{r}
library(brms)
gumbeltree <- custom_family(
  "gumbeltree", dpars = c("mu"), # actually mu is really the sigma for the tree, but the first parameter has to be called sigma,  
                                  # and this is the only parameter that BRMS will be handling
  links = c("log"),

  type = "real",
  #vars = c("y_T","X_T","H_T","C_T","mu_T","K_T","L_T","NW_T","NL_T","T_T","grainsize_T") # the additional parameters to run the tree
  vars = c("grainsize_T")
)

stan_funs_gumbeltree <- "
real partial_sum(real[,] xy_slice,
                   int start,
                   int end,
                   matrix H,
                   vector C,
                   vector mu,
                   real sigma,
                   int K,
                   int L,
                   int NW,
                   int NL,
                   real T) {

  real y[end-start+1] = to_array_1d(xy_slice[,1]);
  matrix[end-start+1 , K] X = to_matrix(xy_slice[,2:(K+1)]);
  matrix[NW,end-start+1] w; //calculation of the weight for each node for each sample
  matrix[NL,end-start+1] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,end-start+1)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,end-start+1) ;
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return normal_lpdf(y | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }


  real gumbeltree_lpdf(real dummy_sample // at the moment there is no data that BRMS is handling
                  , real sigma
                  , vector y
                  , matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , real T
                  , int grainsize) {
  // we use this function when the reduce sum is our target. This may not be easy right now with BRMS
  real F;
  F  = reduce_sum(partial_sum,
                       to_array_2d(append_col(y,X)),
                       grainsize,
                       H,
                       C,
                       mu,
                       sigma,
                       K,
                       L,
                       NW,
                       NL,
                       T);
  return F;  
  }

real gumbeltree_noreduce_lpdf(vector y
                  , real sigma
                  , matrix X
                  , matrix H
                  , vector C
                  , vector mu
                  , int K
                  , int L
                  , int NW
                  , int NL
                  , real T
                  , int grainsize) {
  // we use this version of the function when using rstan backend in BRMS which can't yet use reduce_sum
  int N = length(y);
  matrix[NW,end-start+1] w; //calculation of the weight for each node for each sample
  matrix[NL,end-start+1] temp; //temporary store of tree calculations
                      // after processing, the first row of temp becomes
                      //the estimation of Y given X i.e. y^=F(X)
  int c;// local integer parameter
  int c2;// local integer parameter

  // this is effectively (approximately) choosing which branch of the tree to traverse, based on a cutoff applied to a selected feature
  w=inv_logit(10*((H*X')+rep_matrix(C,N)));

  // start at the bottom of the tree and work upwards
  // at the end of the loop temp[1] is the result of applying the tree to X[i]
  temp = rep_matrix(mu,N) ;
  
  for (j in L:1){
   c=1;
   for (k in 1:(j-1)){
     c = c*2;// can only make integer c = (2^(j-1)) inside a loop
   }

   for (k in 1:c){
     c2=1;
     for (l in 1:j){
       c2 = c2*2;// can only make integer c2 = 2^j inside a loop
     }
     temp[k]=w[k+NW-c2+1] .* temp[2*k-1] + (1-w[k+NW-c2+1]) .* temp[2*k]; // cycling through the tree and calculating which leaf is retained based on w
   }
  }

    return normal_lpdf(y | temp[1]', sigma);// assuming that y is a function that ranges (-inf,+inf)
  }

"

stanvars_trf_data = " 
int NL=1;// number of possible leaves
int NW;// number of nodes or weights or choices in the tree

  for (i in 1:L){
    NL = NL*2;// can only make integer 2^L inside a loop
  }

  NW=NL-1;
"
stanvars_par = "
  vector[K] G[NW];//number of gumbel samples needed to select a feature at each node
  simplex[K] P[NW];//probability vector indicating the probability of selecting each feature at each node, each group adds to 1 i.e. simplex

  vector[NW] C; //for each feature selected we need a cutoff point
  vector[NL] mu_T;// mean of Y at each leaf

"

 stanvars_trf_p = "
 matrix[NW,K] H; // one hot to select a feature at each node
 
 for (j in 1:NW){
      H[j] = softmax( (log(P[j])*(T^2 + T +1) / (T+1) + G[j])/T)' ;
      // selects a parameter using the approximate one-hot dot producted against a specific X[i], adds a cutoff constant and applies a logistic to return (~0,~1)
  }
 "
 
 priors_text = "
 G[i]~gumbel(0,1); // each vector of gumbels used to create a one-hot
 C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest
  //P also needs a prior and should have a U shaped dirichlet prior
for (i in 1:NW){
  P[i]~dirichlet(alpha_T);
}
target += reduce_sum(partial_sum,to_array_2d(append_col(y_T,X_T)),grainsize_T,H_T,C_T,mu_T,sigma_T,K_T,L_T,NW_T,NL_T,T_T)"

# we will use the suffix _T to denote parameters for the tree function to avoid accidentally conflicting a parameter with BRMS
# we are essentially shoehorning our own code into BRMS by force, rendering the main part of the model irelevant at this stage.
# At a later stage, we can envisage BRMS code running a different model as well. e.g a GLM with the tree function providing estimates of the missing variables
stanvars_gumbeltree <- stanvar(scode = stan_funs_gumbeltree, block = "functions")+
                           stanvar(scode = stanvars_trf_data, block = "tdata")+
                           stanvar(scode = stanvars_par, block = "parameters")+
                           stanvar(scode = stanvars_trf_p,block = "tparameters")+
                           stanvar(X%>%select(-y), name = "X_T",scode = "  matrix[N,K] X_T;")+# here we add the design matrix X directly to the model
                           stanvar(X$y, name = "y_T",scode = "  vector[N] y_T;")+# here we add the target samples y directly to the model
                           stanvar(length(X$y), name = "N_T",scode = "  int N_T;")+ # number of samples
                           stanvar(length(X[1,])-1, name = "K_T",scode = "  int K_T;")+ # number of features (minus 1 to exclude y)
                           stanvar(4, name = "L_T",scode = "  int L_T;")+ # number of levels in the tree
                           stanvar(1, name = "T_T",scode = "  real T_T;")+ # temperature of the softmax
                           stanvar(0.2, name = "alpha_T",scode = "  real alpha_T;")+ # alpha parameter fo the dirichlet prior for P
                           stanvar(length(X$y)%/%10, name = "grainsize_T",scode = "  int grainsize_T;") #suggested grainsize for the reduce_sum for the tree
   
   # now that I have the componentry to perform a BRMS model with the model. lets call BRMS and ask for the code to see it works
 formula = bf(y ~  1)
 

 stancode =make_stancode(formula
                               , family = gumbeltree
                               , stanvars = stanvars_gumbeltree
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,iter = 200
                               ,warmup = 200
                               ,chains = 2
                               ,refresh =10
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = c(prior("\n G[i]~gumbel(0,1); // each vector of gumbels used to create a one-hot\n C~normal(0,1); //features in the design matrix are normalised so the cutoffs should be normalised I suggest\n  //P also needs a prior and should have a U shaped dirichlet prior\nfor (i in 1:NW){\n  P[i]~dirichlet(alpha_T);\n}\n\n", check = FALSE))
) 

 stancode

```


```{r}

 MLP2_model_adv_poly_res =brm(formula
                               , family = gumbeltree
                               , stanvars = stanvars_gumbeltree
                               ,data = data.frame(y=1) 
                               ,backend = "cmdstanr"
                               ,threads = threading(20)
                               ,iter = 200
                               ,warmup = 200
                               ,chains = 2
                               ,refresh =10
                               ,max_treedepth=20
                               ,adapt_delta =0.99
                               #,inits = "0"
                               #,thin =1
                               ,prior = c(prior(priors_text, check = FALSE))
) 

```
